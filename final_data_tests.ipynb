{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables(at the top for quick access).\n",
    "NUM_TRIALS = 3\n",
    "FOLDS = 5\n",
    "DATA_SIZE = 5000 #5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helpers\n",
    "class Set:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "# splits targets from data, then splits training from testing data\n",
    "def get_training_test_sets(data, training_size=1000, pred_col='y'):\n",
    "    data_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "    data_shuffled_y = pd.DataFrame(data_shuffled[pred_col])\n",
    "    data_shuffled_X = data_shuffled.drop(pred_col, 1)\n",
    "    tr_X = data_shuffled_X.iloc[:training_size, :].to_numpy()\n",
    "    tr_y = data_shuffled_y.iloc[:training_size, :].values.ravel()\n",
    "    tst_X = data_shuffled_X.iloc[training_size:, :].to_numpy()\n",
    "    tst_y = data_shuffled_y.iloc[training_size:, :].values.ravel()\n",
    "\n",
    "    training = Set(tr_X, tr_y)\n",
    "    testing = Set(tst_X, tst_y)\n",
    "    \n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(raw_data,\n",
    "               column_names=None,\n",
    "               binary_cols=None,\n",
    "               one_hot_cols=None,\n",
    "               continuous_cols=None,\n",
    "              ):\n",
    "    \n",
    "    final_data = raw_data\n",
    "    \n",
    "    if column_names is not None:\n",
    "        final_data.columns = column_names\n",
    "        \n",
    "        if binary_cols is not None:\n",
    "            for col in binary_cols:\n",
    "                if type(col) is tuple:\n",
    "                    match = col[1]\n",
    "                    ind = col[0]\n",
    "                else:\n",
    "                    match = final_data[col].unique()[0]\n",
    "                    ind = col\n",
    "                final_data[ind] = (final_data[ind] != match).astype(int)\n",
    "                \n",
    "        if one_hot_cols is not None:\n",
    "            final_data = pd.get_dummies(final_data, columns=one_hot_cols)\n",
    "            \n",
    "        if final_data.isna().values.any():\n",
    "            print('Warning!: missing data')\n",
    "            \n",
    "        if continuous_cols is not None:\n",
    "            col_names = final_data.columns\n",
    "            mask = np.isin(col_names, continuous_cols, invert=True)\n",
    "            not_continuous = col_names[mask]\n",
    "            \n",
    "            reordered_cols = np.concatenate((continuous_cols, not_continuous))\n",
    "            final_data = final_data[reordered_cols]\n",
    "            \n",
    "            # Normalize\n",
    "            ct = ColumnTransformer([\n",
    "                ('continuous', StandardScaler(), continuous_cols)\n",
    "                \n",
    "            ], remainder='passthrough')\n",
    "            \n",
    "            scaled = ct.fit_transform(final_data)\n",
    "            final_data = pd.DataFrame(scaled, columns=reordered_cols)\n",
    "            \n",
    "    else:\n",
    "        print('No columns names, returning raw data.')\n",
    "        \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set\n",
    "adults_raw = pd.read_csv('data/adults/adult.data', header=None)\n",
    "\n",
    "eye_arff = arff.loadarff('data/eeg_eye/EEG_Eye_State.arff')\n",
    "eyes_raw = pd.DataFrame(eye_arff[0])\n",
    "\n",
    "covertype_raw = pd.read_csv('data/covertype/covtype.data', header=None)\n",
    "\n",
    "\n",
    "\n",
    "adult_process_params = {\n",
    "    'column_names': ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'y'],\n",
    "    'binary_cols': ['sex', 'y'],\n",
    "    'one_hot_cols': ['workclass', 'education', 'marital_status', 'occupation','relationship', 'race', 'native_country'],\n",
    "    'continuous_cols': ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'],\n",
    "}\n",
    "\n",
    "eyes_process_params = {\n",
    "    'column_names': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4', 'y'],\n",
    "    'binary_cols': ['y'],\n",
    "    'continuous_cols': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "}\n",
    "\n",
    "# [('y', 2)], # lodgepole pine\n",
    "covtyp_process_params = {\n",
    "    'column_names': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type0', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'y'],\n",
    "    'binary_cols': [('y', 2)],\n",
    "    'continuous_cols': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'],\n",
    "}\n",
    "# Cover Type column names can be rebuilt with the code below if need be.\n",
    "# cov_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
    "# Wilderness_Area = ['Wilderness_Area_{}'.format(i) for i in range(4)]\n",
    "# Soil_Type = ['Soil_Type{}'.format(i) for i in range(40)]\n",
    "# cov_cols = np.concatenate((cov_cols, Wilderness_Area, Soil_Type, ['y']))\n",
    "# covertype_raw.columns = cov_cols\n",
    "\n",
    "adults = clean_data(adults_raw, **adult_process_params)\n",
    "eyes = clean_data(eyes_raw, **eyes_process_params)\n",
    "pines = clean_data(covertype_raw, **covtyp_process_params)\n",
    "# print(adults.head())\n",
    "# print(eyes.head())\n",
    "# print(pines.head())\n",
    "\n",
    "all_data = [adults, eyes, pines]\n",
    "data_names = ['ADULTS', 'EEG_EYE', 'COV_TYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM, #ATTR, TRAIN SIZE, TEST SIZE, %POZ\n",
      "ADULTS, 107, 5000, 32561, 24%\n",
      "EEG_EYE, 14, 5000, 14980, 44%\n",
      "COV_TYPE, 54, 5000, 581012, 51%\n"
     ]
    }
   ],
   "source": [
    "# CNM06 Table 1\n",
    "print('PROBLEM, #ATTR, TRAIN SIZE, TEST SIZE, %POZ')\n",
    "for i in range(len(all_data)):\n",
    "    d = all_data[i]\n",
    "    name = data_names[i]\n",
    "    features = (len(d.columns)-1)\n",
    "    entries = len(d)\n",
    "    pos = 100 * d['y'].sum() / entries\n",
    "    print('{}, {}, 5000, {}, {}%'.format(name, features, entries, int(pos)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Starting: SVM\n",
      "========================\n",
      "\n",
      "  Starting data set ADULTS...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "    Training Results (outer)\n",
      "    >acc=0.861, est=0.854, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.839, est=0.860, cfg={'C': 10, 'gamma': 0.01}\n",
      "    >acc=0.868, est=0.848, cfg={'C': 10, 'gamma': 0.01}\n",
      "    >acc=0.868, est=0.849, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.845, est=0.856, cfg={'C': 1000, 'gamma': 0.001}\n",
      "    Overall Training Acc: 0.856 std(0.012)\n",
      "  Training Master for test: {'C': 10, 'gamma': 0.01}\n",
      "  Test_Set Acc = 0.852 \n",
      "\n",
      "  Trial 2...\n",
      "    Training Results (outer)\n",
      "    >acc=0.853, est=0.860, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.843, est=0.863, cfg={'C': 100, 'gamma': 0.001}\n",
      "    >acc=0.875, est=0.859, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.872, est=0.857, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.847, est=0.863, cfg={'C': 1, 'gamma': 0.1}\n",
      "    Overall Training Acc: 0.858 std(0.013)\n",
      "  Training Master for test: {'C': 1, 'gamma': 0.1}\n",
      "  Test_Set Acc = 0.851 \n",
      "\n",
      "  Trial 3...\n",
      "    Training Results (outer)\n",
      "    >acc=0.849, est=0.845, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.841, est=0.849, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.854, est=0.843, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.844, est=0.846, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.840, est=0.846, cfg={'C': 10, 'gamma': 0.01}\n",
      "    Overall Training Acc: 0.846 std(0.005)\n",
      "  Training Master for test: {'C': 1, 'gamma': 0.1}\n",
      "  Test_Set Acc = 0.854 \n",
      "\n",
      "  Starting data set EEG_EYE...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "    Training Results (outer)\n",
      "    >acc=0.834, est=0.816, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    >acc=0.815, est=0.820, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    >acc=0.849, est=0.816, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    >acc=0.825, est=0.827, cfg={'C': 100, 'gamma': 1.0}\n",
      "    >acc=0.825, est=0.824, cfg={'C': 100, 'gamma': 1.0}\n",
      "    Overall Training Acc: 0.830 std(0.011)\n",
      "  Training Master for test: {'C': 1000, 'gamma': 1.0}\n",
      "  Test_Set Acc = 0.849 \n",
      "\n",
      "  Trial 2...\n",
      "    Training Results (outer)\n",
      "    >acc=0.841, est=0.813, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    >acc=0.834, est=0.817, cfg={'C': 100, 'gamma': 1.0}\n",
      "    >acc=0.818, est=0.825, cfg={'C': 100, 'gamma': 1.0}\n",
      "    >acc=0.838, est=0.811, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    >acc=0.822, est=0.819, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    Overall Training Acc: 0.831 std(0.009)\n",
      "  Training Master for test: {'C': 1000, 'gamma': 1.0}\n",
      "  Test_Set Acc = 0.847 \n",
      "\n",
      "  Trial 3...\n",
      "    Training Results (outer)\n",
      "    >acc=0.834, est=0.826, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    >acc=0.853, est=0.825, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    >acc=0.828, est=0.825, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    >acc=0.839, est=0.835, cfg={'C': 100, 'gamma': 1.0}\n",
      "    >acc=0.839, est=0.831, cfg={'C': 1000, 'gamma': 1.0}\n",
      "    Overall Training Acc: 0.839 std(0.008)\n",
      "  Training Master for test: {'C': 1000, 'gamma': 1.0}\n",
      "  Test_Set Acc = 0.845 \n",
      "\n",
      "  Starting data set COV_TYPE...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "    Training Results (outer)\n",
      "    >acc=0.786, est=0.792, cfg={'C': 1, 'gamma': 0.1}\n",
      "    >acc=0.811, est=0.787, cfg={'C': 10, 'gamma': 0.1}\n",
      "    >acc=0.789, est=0.790, cfg={'C': 1000, 'gamma': 0.01}\n",
      "    >acc=0.779, est=0.789, cfg={'C': 10, 'gamma': 0.1}\n",
      "    >acc=0.811, est=0.783, cfg={'C': 1, 'gamma': 0.1}\n",
      "    Overall Training Acc: 0.795 std(0.013)\n",
      "  Training Master for test: {'C': 10, 'gamma': 0.1}\n",
      "  Test_Set Acc = 0.802 \n",
      "\n",
      "  Trial 2...\n",
      "    Training Results (outer)\n",
      "    >acc=0.803, est=0.784, cfg={'C': 10, 'gamma': 0.1}\n",
      "    >acc=0.804, est=0.788, cfg={'C': 10, 'gamma': 0.1}\n",
      "    >acc=0.788, est=0.783, cfg={'C': 1000, 'gamma': 0.01}\n",
      "    >acc=0.791, est=0.784, cfg={'C': 10, 'gamma': 0.1}\n",
      "    >acc=0.789, est=0.786, cfg={'C': 1000, 'gamma': 0.01}\n",
      "    Overall Training Acc: 0.795 std(0.007)\n",
      "  Training Master for test: {'C': 10, 'gamma': 0.1}\n",
      "  Test_Set Acc = 0.806 \n",
      "\n",
      "  Trial 3...\n",
      "    Training Results (outer)\n",
      "    >acc=0.797, est=0.787, cfg={'C': 10, 'gamma': 0.1}\n",
      "    >acc=0.790, est=0.788, cfg={'C': 10, 'gamma': 0.1}\n",
      "    >acc=0.794, est=0.795, cfg={'C': 10, 'gamma': 0.1}\n",
      "    >acc=0.784, est=0.790, cfg={'C': 10, 'gamma': 0.1}\n",
      "    >acc=0.799, est=0.789, cfg={'C': 10, 'gamma': 0.1}\n",
      "    Overall Training Acc: 0.793 std(0.005)\n",
      "  Training Master for test: {'C': 10, 'gamma': 0.1}\n",
      "  Test_Set Acc = 0.803 \n",
      "\n",
      "========================\n",
      "Starting: RF\n",
      "========================\n",
      "\n",
      "  Starting data set ADULTS...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "    Training Results (outer)\n",
      "    >acc=0.865, est=0.863, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "    >acc=0.848, est=0.864, cfg={'max_depth': 10, 'max_features': 16, 'n_estimators': 100}\n",
      "    >acc=0.859, est=0.862, cfg={'max_depth': 10, 'max_features': 16, 'n_estimators': 100}\n",
      "    >acc=0.856, est=0.866, cfg={'max_depth': None, 'max_features': 20, 'n_estimators': 100}\n",
      "    >acc=0.858, est=0.859, cfg={'max_depth': None, 'max_features': 8, 'n_estimators': 100}\n",
      "    Overall Training Acc: 0.857 std(0.005)\n",
      "  Training Master for test: {'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "  Test_Set Acc = 0.856 \n",
      "\n",
      "  Trial 2...\n",
      "    Training Results (outer)\n",
      "    >acc=0.860, est=0.863, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "    >acc=0.861, est=0.864, cfg={'max_depth': 10, 'max_features': 18, 'n_estimators': 100}\n",
      "    >acc=0.844, est=0.866, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "    >acc=0.885, est=0.858, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "    >acc=0.874, est=0.859, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "    Overall Training Acc: 0.865 std(0.014)\n",
      "  Training Master for test: {'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "  Test_Set Acc = 0.857 \n",
      "\n",
      "  Trial 3...\n",
      "    Training Results (outer)\n",
      "    >acc=0.848, est=0.854, cfg={'max_depth': 10, 'max_features': 18, 'n_estimators': 100}\n",
      "    >acc=0.856, est=0.849, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "    >acc=0.839, est=0.856, cfg={'max_depth': 10, 'max_features': 18, 'n_estimators': 100}\n",
      "    >acc=0.863, est=0.848, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "    >acc=0.847, est=0.855, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "    Overall Training Acc: 0.851 std(0.008)\n",
      "  Training Master for test: {'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "  Test_Set Acc = 0.858 \n",
      "\n",
      "  Starting data set EEG_EYE...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "    Training Results (outer)\n",
      "    >acc=0.875, est=0.877, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "    >acc=0.892, est=0.881, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "    >acc=0.889, est=0.875, cfg={'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "    >acc=0.890, est=0.876, cfg={'max_depth': None, 'max_features': 2, 'n_estimators': 100}\n",
      "    >acc=0.894, est=0.874, cfg={'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "    Overall Training Acc: 0.888 std(0.007)\n",
      "  Training Master for test: {'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "  Test_Set Acc = 0.893 \n",
      "\n",
      "  Trial 2...\n",
      "    Training Results (outer)\n",
      "    >acc=0.880, est=0.879, cfg={'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "    >acc=0.896, est=0.880, cfg={'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "    >acc=0.895, est=0.878, cfg={'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "    >acc=0.882, est=0.880, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "    >acc=0.881, est=0.876, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "    Overall Training Acc: 0.887 std(0.007)\n",
      "  Training Master for test: {'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "  Test_Set Acc = 0.899 \n",
      "\n",
      "  Trial 3...\n",
      "    Training Results (outer)\n",
      "    >acc=0.914, est=0.883, cfg={'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "    >acc=0.903, est=0.882, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "    >acc=0.890, est=0.878, cfg={'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "    >acc=0.881, est=0.891, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "    >acc=0.893, est=0.886, cfg={'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "    Overall Training Acc: 0.896 std(0.011)\n",
      "  Training Master for test: {'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test_Set Acc = 0.896 \n",
      "\n",
      "  Starting data set COV_TYPE...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "    Training Results (outer)\n",
      "    >acc=0.797, est=0.810, cfg={'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "    >acc=0.820, est=0.800, cfg={'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "    >acc=0.818, est=0.803, cfg={'max_depth': None, 'max_features': 2, 'n_estimators': 100}\n",
      "    >acc=0.811, est=0.810, cfg={'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "    >acc=0.811, est=0.807, cfg={'max_depth': None, 'max_features': 2, 'n_estimators': 100}\n",
      "    Overall Training Acc: 0.811 std(0.008)\n",
      "  Training Master for test: {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "  Test_Set Acc = 0.816 \n",
      "\n",
      "  Trial 2...\n",
      "    Training Results (outer)\n",
      "    >acc=0.824, est=0.802, cfg={'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "    >acc=0.807, est=0.802, cfg={'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "    >acc=0.799, est=0.804, cfg={'max_depth': 100, 'max_features': 18, 'n_estimators': 100}\n",
      "    >acc=0.808, est=0.802, cfg={'max_depth': 100, 'max_features': 16, 'n_estimators': 100}\n",
      "    >acc=0.810, est=0.809, cfg={'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "    Overall Training Acc: 0.810 std(0.008)\n",
      "  Training Master for test: {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "  Test_Set Acc = 0.818 \n",
      "\n",
      "  Trial 3...\n",
      "    Training Results (outer)\n",
      "    >acc=0.801, est=0.809, cfg={'max_depth': 100, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "    >acc=0.815, est=0.804, cfg={'max_depth': 100, 'max_features': 18, 'n_estimators': 100}\n",
      "    >acc=0.815, est=0.811, cfg={'max_depth': 100, 'max_features': 18, 'n_estimators': 100}\n",
      "    >acc=0.794, est=0.809, cfg={'max_depth': None, 'max_features': 18, 'n_estimators': 100}\n",
      "    >acc=0.816, est=0.808, cfg={'max_depth': 100, 'max_features': 36, 'n_estimators': 100}\n",
      "    Overall Training Acc: 0.808 std(0.009)\n",
      "  Training Master for test: {'max_depth': 100, 'max_features': 36, 'n_estimators': 100}\n",
      "  Test_Set Acc = 0.822 \n",
      "\n",
      "========================\n",
      "Starting: gNB\n",
      "========================\n",
      "\n",
      "  Starting data set ADULTS...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "    Training Results (outer)\n",
      "    >acc=0.531, est=0.519, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.498, est=0.491, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.521, est=0.545, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.522, est=0.520, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.524, est=0.504, cfg={'var_smoothing': 1e-05}\n",
      "    Overall Training Acc: 0.519 std(0.011)\n",
      "  Training Master for test: {'var_smoothing': 1e-05}\n",
      "  Test_Set Acc = 0.526 \n",
      "\n",
      "  Trial 2...\n",
      "    Training Results (outer)\n",
      "    >acc=0.570, est=0.549, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.538, est=0.532, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.530, est=0.532, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.514, est=0.542, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.560, est=0.538, cfg={'var_smoothing': 1e-05}\n",
      "    Overall Training Acc: 0.542 std(0.020)\n",
      "  Training Master for test: {'var_smoothing': 1e-05}\n",
      "  Test_Set Acc = 0.545 \n",
      "\n",
      "  Trial 3...\n",
      "    Training Results (outer)\n",
      "    >acc=0.584, est=0.580, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.574, est=0.570, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.587, est=0.574, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.564, est=0.538, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.543, est=0.543, cfg={'var_smoothing': 1e-05}\n",
      "    Overall Training Acc: 0.570 std(0.016)\n",
      "  Training Master for test: {'var_smoothing': 1e-05}\n",
      "  Test_Set Acc = 0.569 \n",
      "\n",
      "  Starting data set EEG_EYE...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "    Training Results (outer)\n",
      "    >acc=0.618, est=0.608, cfg={'var_smoothing': 1e-06}\n",
      "    >acc=0.619, est=0.597, cfg={'var_smoothing': 1e-07}\n",
      "    >acc=0.645, est=0.614, cfg={'var_smoothing': 1e-08}\n",
      "    >acc=0.629, est=0.617, cfg={'var_smoothing': 1e-08}\n",
      "    >acc=0.560, est=0.608, cfg={'var_smoothing': 1e-07}\n",
      "    Overall Training Acc: 0.614 std(0.029)\n",
      "  Training Master for test: {'var_smoothing': 1e-08}\n",
      "  Test_Set Acc = 0.609 \n",
      "\n",
      "  Trial 2...\n",
      "    Training Results (outer)\n",
      "    >acc=0.453, est=0.480, cfg={'var_smoothing': 1e-07}\n",
      "    >acc=0.570, est=0.566, cfg={'var_smoothing': 1e-08}\n",
      "    >acc=0.548, est=0.565, cfg={'var_smoothing': 1e-07}\n",
      "    >acc=0.560, est=0.564, cfg={'var_smoothing': 1e-09}\n",
      "    >acc=0.557, est=0.535, cfg={'var_smoothing': 1e-07}\n",
      "    Overall Training Acc: 0.538 std(0.043)\n",
      "  Training Master for test: {'var_smoothing': 1e-08}\n",
      "  Test_Set Acc = 0.551 \n",
      "\n",
      "  Trial 3...\n",
      "    Training Results (outer)\n",
      "    >acc=0.468, est=0.478, cfg={'var_smoothing': 1e-07}\n",
      "    >acc=0.466, est=0.486, cfg={'var_smoothing': 1e-08}\n",
      "    >acc=0.420, est=0.494, cfg={'var_smoothing': 1e-07}\n",
      "    >acc=0.601, est=0.626, cfg={'var_smoothing': 1e-08}\n",
      "    >acc=0.465, est=0.481, cfg={'var_smoothing': 1e-07}\n",
      "    Overall Training Acc: 0.484 std(0.061)\n",
      "  Training Master for test: {'var_smoothing': 1e-08}\n",
      "  Test_Set Acc = 0.457 \n",
      "\n",
      "  Starting data set COV_TYPE...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "    Training Results (outer)\n",
      "    >acc=0.629, est=0.639, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.674, est=0.659, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.671, est=0.659, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.643, est=0.663, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.654, est=0.662, cfg={'var_smoothing': 1e-05}\n",
      "    Overall Training Acc: 0.654 std(0.017)\n",
      "  Training Master for test: {'var_smoothing': 1e-05}\n",
      "  Test_Set Acc = 0.662 \n",
      "\n",
      "  Trial 2...\n",
      "    Training Results (outer)\n",
      "    >acc=0.673, est=0.660, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.652, est=0.656, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.632, est=0.642, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.617, est=0.639, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.667, est=0.667, cfg={'var_smoothing': 1e-05}\n",
      "    Overall Training Acc: 0.648 std(0.021)\n",
      "  Training Master for test: {'var_smoothing': 1e-05}\n",
      "  Test_Set Acc = 0.629 \n",
      "\n",
      "  Trial 3...\n",
      "    Training Results (outer)\n",
      "    >acc=0.626, est=0.625, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.611, est=0.626, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.663, est=0.622, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.633, est=0.642, cfg={'var_smoothing': 1e-05}\n",
      "    >acc=0.605, est=0.614, cfg={'var_smoothing': 1e-05}\n",
      "    Overall Training Acc: 0.628 std(0.020)\n",
      "  Training Master for test: {'var_smoothing': 1e-05}\n",
      "  Test_Set Acc = 0.647 \n",
      "\n",
      "CPU times: user 8min 11s, sys: 8.52 s, total: 8min 20s\n",
      "Wall time: 37min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifiers = [\n",
    "    {\n",
    "        'name': 'SVM',\n",
    "        'method': SVC,\n",
    "        'p_grid': {'C': [1,10,100,1000], 'gamma': [0.001,0.01,0.1,1.0]},\n",
    "        'test_acc': list()\n",
    "    },\n",
    "    {\n",
    "        'name': 'RF',\n",
    "        'method': RandomForestClassifier,\n",
    "        'p_grid': {'n_estimators': [100], 'max_features': ['sqrt',2,5,8,16,18,20,36], 'max_depth': [10,100,None]},\n",
    "        'test_acc': list()\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'name': 'gNB',\n",
    "        'method': GaussianNB,\n",
    "        'p_grid': {'var_smoothing': [1.0e-5, 1.0e-6, 1.0e-7, 1.0e-8, 1.0e-9]},\n",
    "        'test_acc': list()\n",
    "    }\n",
    "]\n",
    "\n",
    "gridcvs = {\n",
    "    'SVM': {},\n",
    "    'RF': {},\n",
    "    'gNB': {},\n",
    "}\n",
    "\n",
    "tst_models = {\n",
    "    'SVM': {},\n",
    "    'RF': {},\n",
    "    'gNB': {},\n",
    "}\n",
    "\n",
    "dbs_acc = {}\n",
    "for name in data_names:\n",
    "    dbs_acc[name] = []\n",
    "\n",
    "tbl_2 = []\n",
    "tbl_3 = []\n",
    "    \n",
    "for classifier in classifiers:\n",
    "    print('========================')\n",
    "    print('Starting: {}'.format(classifier['name']))\n",
    "    print('========================\\n')\n",
    "    \n",
    "    clf = classifier['method']()\n",
    "    p_grid = classifier['p_grid']\n",
    "    \n",
    "    for d_i in range(len(all_data)):\n",
    "        data_set = all_data[d_i]\n",
    "        name = data_names[d_i]\n",
    "        \n",
    "        gridcvs[classifier['name']][name] = []\n",
    "        tst_models[classifier['name']][name] = []\n",
    "        \n",
    "        print('  Starting data set {}...'.format(name))\n",
    "        print('  ~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "        test_set_scores = []\n",
    "        for i in range(NUM_TRIALS):\n",
    "\n",
    "            training_set, testing_set = get_training_test_sets(data_set, DATA_SIZE, pred_col='y')\n",
    "\n",
    "            print('  Trial {}...'.format(i+1))\n",
    "            trial_results = []\n",
    "            outer_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "            best_p = []\n",
    "            best_score = []\n",
    "\n",
    "            print('    Training Results (outer)')\n",
    "            for tr_i, tst_i in outer_cv.split(training_set.X):\n",
    "                X_train, X_test = training_set.X[tr_i, :], training_set.X[tst_i, :]\n",
    "                y_train, y_test = training_set.y[tr_i], training_set.y[tst_i]\n",
    "\n",
    "                inner_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "\n",
    "\n",
    "\n",
    "                search = GridSearchCV(\n",
    "                    estimator=clf,\n",
    "                    param_grid=p_grid,\n",
    "                    cv=inner_cv,\n",
    "                    verbose=0,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    refit=True\n",
    "                )\n",
    "\n",
    "                result = search.fit(X_train, y_train)\n",
    "                \n",
    "                gridcvs[classifier['name']][name].append(result)\n",
    "                \n",
    "                model = result.best_estimator_\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                best_p.append(result.best_params_)\n",
    "                best_score.append(acc)\n",
    "                trial_results.append(acc)\n",
    "\n",
    "                print('    >acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "\n",
    "\n",
    "            \n",
    "            print('    Overall Training Acc: %.3f std(%.3f)' % (np.mean(trial_results), np.std(trial_results)))\n",
    "\n",
    "            run = best_score.index(max(best_score))\n",
    "            best_best_p = best_p[run]\n",
    "\n",
    "            oo_clf = classifier['method']()\n",
    "            print('  Training Master for test: %s' % best_best_p)\n",
    "            oo_clf.set_params(**best_best_p)\n",
    "            outer_results = oo_clf.fit(training_set.X,training_set.y)\n",
    "            \n",
    "            tst_models[classifier['name']][name].append(outer_results)\n",
    "            \n",
    "            y_pred = oo_clf.predict(testing_set.X)\n",
    "            acc = accuracy_score(testing_set.y, y_pred)\n",
    "            classifier['test_acc'].append(acc)\n",
    "            dbs_acc[name].append(acc)\n",
    "            test_set_scores.append(acc)\n",
    "            print('  Test_Set Acc = %.3f \\n' % acc)\n",
    "            \n",
    "        tbl_2.append((classifier['name'], np.mean(test_set_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL, ACC\n",
      "SVM, 0.852\n",
      "SVM, 0.847\n",
      "SVM, 0.804\n",
      "RF, 0.857\n",
      "RF, 0.896\n",
      "RF, 0.819\n",
      "gNB, 0.547\n",
      "gNB, 0.539\n",
      "gNB, 0.646\n"
     ]
    }
   ],
   "source": [
    "# table 2\n",
    "print('MODEL, ACC')\n",
    "for name, val in tbl_2:\n",
    "    print('{}, {:.3f}'.format(name, val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL, ADULTS, EEG_EYE, COV_TYPE, MEAN\n",
      "SVM, 0.852, 0.847, 0.804, 0.834\n",
      "RF, 0.857, 0.896, 0.819, 0.857\n",
      "gNB, 0.547, 0.539, 0.646, 0.577\n"
     ]
    }
   ],
   "source": [
    "# table 3\n",
    "quick_stack = {\n",
    "    'SVM': [],\n",
    "    'RF': [],\n",
    "    'gNB': [],\n",
    "}\n",
    "\n",
    "print('MODEL, ADULTS, EEG_EYE, COV_TYPE, MEAN')\n",
    "for el in tbl_2:\n",
    "    quick_stack[el[0]].append(el[1])\n",
    "    \n",
    "for key, values in quick_stack.items():\n",
    "    val_str = ['{:.3f}'.format(x) for x in values]\n",
    "    print('{}, {}, {:.3f}'.format(key, \", \".join(val_str), np.mean(values)))\n",
    "#     for val in values:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo_1, Algo_2, statistic, p-value\n",
      "SVM, RF, -1.698, 0.111\n",
      "SVM, gNB, 11.063, 6.46e-07\n",
      "RF, gNB, 11.395, 9.51e-08\n"
     ]
    }
   ],
   "source": [
    "# t-statistic and p-value table\n",
    "test_perm = combinations(classifiers, 2)\n",
    "print('Algo_1, Algo_2, statistic, p-value')\n",
    "for pair in list(test_perm):\n",
    "    a = pair[0]\n",
    "    b = pair[1]\n",
    "    \n",
    "    t,p = stats.ttest_ind(a['test_acc'],b['test_acc'], equal_var=False)\n",
    "    print('{}, {}, {:.3f}, {:.3g}'.format(a['name'], b['name'], t, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keepin' the  books\n",
    "\n",
    "# Models actually used for test set predictions\n",
    "# print(type(tst_models['SVM']['ADULTS'][0]))\n",
    "filename = 'all_test_models.sav'\n",
    "pickle.dump(tst_models, open(filename, 'wb'))\n",
    "\n",
    "# load the tst_models\n",
    "# tst_models = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# GridSearchCV results\n",
    "f = open(\"grid_results.txt\", \"w\")\n",
    "for key, clf_arr in gridcvs.items():\n",
    "    for d_name, d_set in clf_arr.items():\n",
    "        for fold in d_set:\n",
    "#             print(fold.cv_results_)\n",
    "            f.write('{} - {}:\\n-------------\\n'.format(key,d_name))\n",
    "            f.write(str(fold.cv_results_))\n",
    "            f.write(\"\\n\\n===================================\\n\")\n",
    "            f.write(\"===================================\\n\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
