{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables(at the top for quick access).\n",
    "NUM_TRIALS = 3\n",
    "FOLDS = 5\n",
    "DATA_SIZE = 5000 #5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helpers\n",
    "class Set:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "# splits targets from data, then splits training from testing data\n",
    "def get_training_test_sets(data, training_size=1000, pred_col='y'):\n",
    "    data_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "    data_shuffled_y = pd.DataFrame(data_shuffled[pred_col])\n",
    "    data_shuffled_X = data_shuffled.drop(pred_col, 1)\n",
    "    tr_X = data_shuffled_X.iloc[:training_size, :].to_numpy()\n",
    "    tr_y = data_shuffled_y.iloc[:training_size, :].values.ravel()\n",
    "    tst_X = data_shuffled_X.iloc[training_size:, :].to_numpy()\n",
    "    tst_y = data_shuffled_y.iloc[training_size:, :].values.ravel()\n",
    "\n",
    "    training = Set(tr_X, tr_y)\n",
    "    testing = Set(tst_X, tst_y)\n",
    "    \n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(raw_data,\n",
    "               column_names=None,\n",
    "               binary_cols=None,\n",
    "               one_hot_cols=None,\n",
    "               continuous_cols=None,\n",
    "              ):\n",
    "    \n",
    "    final_data = raw_data\n",
    "    \n",
    "    if column_names is not None:\n",
    "        final_data.columns = column_names\n",
    "        \n",
    "        if binary_cols is not None:\n",
    "            for col in binary_cols:\n",
    "                if type(col) is tuple:\n",
    "                    match = col[1]\n",
    "                    ind = col[0]\n",
    "                else:\n",
    "                    match = final_data[col].unique()[0]\n",
    "                    ind = col\n",
    "                final_data[ind] = (final_data[ind] != match).astype(int)\n",
    "                \n",
    "        if one_hot_cols is not None:\n",
    "            final_data = pd.get_dummies(final_data, columns=one_hot_cols)\n",
    "            \n",
    "        if final_data.isna().values.any():\n",
    "            print('Warning!: missing data')\n",
    "            \n",
    "        if continuous_cols is not None:\n",
    "            col_names = final_data.columns\n",
    "            mask = np.isin(col_names, continuous_cols, invert=True)\n",
    "            not_continuous = col_names[mask]\n",
    "            \n",
    "            reordered_cols = np.concatenate((continuous_cols, not_continuous))\n",
    "            final_data = final_data[reordered_cols]\n",
    "            \n",
    "            # Normalize\n",
    "            ct = ColumnTransformer([\n",
    "                ('continuous', StandardScaler(), continuous_cols)\n",
    "                \n",
    "            ], remainder='passthrough')\n",
    "            \n",
    "            scaled = ct.fit_transform(final_data)\n",
    "            final_data = pd.DataFrame(scaled, columns=reordered_cols)\n",
    "            \n",
    "    else:\n",
    "        print('No columns names, returning raw data.')\n",
    "        \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set\n",
    "adults_raw = pd.read_csv('data/adults/adult.data', header=None)\n",
    "\n",
    "eye_arff = arff.loadarff('data/eeg_eye/EEG_Eye_State.arff')\n",
    "eyes_raw = pd.DataFrame(eye_arff[0])\n",
    "\n",
    "covertype_raw = pd.read_csv('data/covertype/covtype.data', header=None)\n",
    "\n",
    "\n",
    "\n",
    "adult_process_params = {\n",
    "    'column_names': ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'y'],\n",
    "    'binary_cols': ['sex', 'y'],\n",
    "    'one_hot_cols': ['workclass', 'education', 'marital_status', 'occupation','relationship', 'race', 'native_country'],\n",
    "    'continuous_cols': ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'],\n",
    "}\n",
    "\n",
    "eyes_process_params = {\n",
    "    'column_names': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4', 'y'],\n",
    "    'binary_cols': ['y'],\n",
    "    'continuous_cols': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "}\n",
    "\n",
    "# [('y', 2)], # lodgepole pine\n",
    "covtyp_process_params = {\n",
    "    'column_names': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type0', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'y'],\n",
    "    'binary_cols': [('y', 2)],\n",
    "    'continuous_cols': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'],\n",
    "}\n",
    "# Cover Type column names can be rebuilt with the code below if need be.\n",
    "# cov_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
    "# Wilderness_Area = ['Wilderness_Area_{}'.format(i) for i in range(4)]\n",
    "# Soil_Type = ['Soil_Type{}'.format(i) for i in range(40)]\n",
    "# cov_cols = np.concatenate((cov_cols, Wilderness_Area, Soil_Type, ['y']))\n",
    "# covertype_raw.columns = cov_cols\n",
    "\n",
    "adults = clean_data(adults_raw, **adult_process_params)\n",
    "eyes = clean_data(eyes_raw, **eyes_process_params)\n",
    "pines = clean_data(covertype_raw, **covtyp_process_params)\n",
    "# print(adults.head())\n",
    "# print(eyes.head())\n",
    "# print(pines.head())\n",
    "\n",
    "all_data = [adults, eyes, pines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: SVC\n",
      "~~~~~~~~~~~\n",
      "\n",
      "========================\n",
      "Starting data set 1...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.865, est=0.847, cfg={'C': 1, 'gamma': 0.1}\n",
      ">acc=0.845, est=0.851, cfg={'C': 1000, 'gamma': 0.001}\n",
      ">acc=0.850, est=0.849, cfg={'C': 1000, 'gamma': 0.001}\n",
      ">acc=0.829, est=0.855, cfg={'C': 10, 'gamma': 0.01}\n",
      ">acc=0.841, est=0.851, cfg={'C': 1000, 'gamma': 0.001}\n",
      "Accuracy: 0.846 (0.012)\n",
      "Training Master: {'C': 1, 'gamma': 0.1}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.853 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.844, est=0.855, cfg={'C': 10, 'gamma': 0.01}\n",
      ">acc=0.851, est=0.859, cfg={'C': 10, 'gamma': 0.01}\n",
      ">acc=0.852, est=0.855, cfg={'C': 10, 'gamma': 0.01}\n",
      ">acc=0.865, est=0.854, cfg={'C': 1000, 'gamma': 0.001}\n",
      ">acc=0.863, est=0.857, cfg={'C': 1000, 'gamma': 0.001}\n",
      "Accuracy: 0.855 (0.008)\n",
      "Training Master: {'C': 1000, 'gamma': 0.001}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.852 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.848, est=0.859, cfg={'C': 1, 'gamma': 0.1}\n",
      ">acc=0.849, est=0.861, cfg={'C': 10, 'gamma': 0.01}\n",
      ">acc=0.859, est=0.856, cfg={'C': 100, 'gamma': 0.001}\n",
      ">acc=0.854, est=0.861, cfg={'C': 1, 'gamma': 0.1}\n",
      ">acc=0.862, est=0.861, cfg={'C': 10, 'gamma': 0.01}\n",
      "Accuracy: 0.854 (0.005)\n",
      "Training Master: {'C': 10, 'gamma': 0.01}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.850 \n",
      "\n",
      "========================\n",
      "Starting data set 2...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.821, est=0.826, cfg={'C': 1000, 'gamma': 1.0}\n",
      ">acc=0.844, est=0.815, cfg={'C': 1000, 'gamma': 1.0}\n",
      ">acc=0.828, est=0.828, cfg={'C': 1000, 'gamma': 1.0}\n",
      ">acc=0.840, est=0.817, cfg={'C': 1000, 'gamma': 1.0}\n",
      ">acc=0.851, est=0.819, cfg={'C': 1000, 'gamma': 1.0}\n",
      "Accuracy: 0.837 (0.011)\n",
      "Training Master: {'C': 1000, 'gamma': 1.0}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.846 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.833, est=0.820, cfg={'C': 1000, 'gamma': 1.0}\n",
      ">acc=0.839, est=0.816, cfg={'C': 1000, 'gamma': 1.0}\n",
      ">acc=0.832, est=0.824, cfg={'C': 100, 'gamma': 1.0}\n",
      ">acc=0.811, est=0.824, cfg={'C': 100, 'gamma': 1.0}\n",
      ">acc=0.851, est=0.818, cfg={'C': 1000, 'gamma': 1.0}\n",
      "Accuracy: 0.833 (0.013)\n",
      "Training Master: {'C': 1000, 'gamma': 1.0}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.846 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.838, est=0.821, cfg={'C': 1000, 'gamma': 1.0}\n",
      ">acc=0.836, est=0.828, cfg={'C': 1000, 'gamma': 1.0}\n",
      ">acc=0.847, est=0.817, cfg={'C': 100, 'gamma': 1.0}\n",
      ">acc=0.837, est=0.827, cfg={'C': 1000, 'gamma': 1.0}\n",
      ">acc=0.819, est=0.832, cfg={'C': 1000, 'gamma': 1.0}\n",
      "Accuracy: 0.835 (0.009)\n",
      "Training Master: {'C': 100, 'gamma': 1.0}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.840 \n",
      "\n",
      "========================\n",
      "Starting data set 3...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.803, est=0.777, cfg={'C': 10, 'gamma': 0.1}\n",
      ">acc=0.796, est=0.783, cfg={'C': 10, 'gamma': 0.1}\n",
      ">acc=0.801, est=0.782, cfg={'C': 10, 'gamma': 0.1}\n",
      ">acc=0.756, est=0.789, cfg={'C': 1, 'gamma': 1.0}\n",
      ">acc=0.786, est=0.783, cfg={'C': 10, 'gamma': 0.1}\n",
      "Accuracy: 0.788 (0.017)\n",
      "Training Master: {'C': 10, 'gamma': 0.1}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.804 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.814, est=0.804, cfg={'C': 10, 'gamma': 0.1}\n",
      ">acc=0.809, est=0.805, cfg={'C': 10, 'gamma': 0.1}\n",
      ">acc=0.791, est=0.794, cfg={'C': 1, 'gamma': 0.1}\n",
      ">acc=0.769, est=0.806, cfg={'C': 1000, 'gamma': 0.01}\n",
      ">acc=0.814, est=0.803, cfg={'C': 10, 'gamma': 0.1}\n",
      "Accuracy: 0.799 (0.017)\n",
      "Training Master: {'C': 10, 'gamma': 0.1}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.803 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.765, est=0.796, cfg={'C': 10, 'gamma': 0.1}\n",
      ">acc=0.794, est=0.778, cfg={'C': 1000, 'gamma': 0.01}\n",
      ">acc=0.785, est=0.788, cfg={'C': 10, 'gamma': 0.1}\n",
      ">acc=0.797, est=0.783, cfg={'C': 10, 'gamma': 0.1}\n",
      ">acc=0.784, est=0.779, cfg={'C': 1, 'gamma': 0.1}\n",
      "Accuracy: 0.785 (0.011)\n",
      "Training Master: {'C': 10, 'gamma': 0.1}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.801 \n",
      "\n",
      "Starting: RandomForestClassifier\n",
      "~~~~~~~~~~~\n",
      "\n",
      "========================\n",
      "Starting data set 1...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.872, est=0.857, cfg={'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.873, est=0.857, cfg={'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.853, est=0.863, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.848, est=0.865, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.860, est=0.862, cfg={'max_depth': 10, 'max_features': 16, 'n_estimators': 100}\n",
      "Accuracy: 0.861 (0.010)\n",
      "Training Master: {'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.854 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.853, est=0.870, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.874, est=0.868, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.868, est=0.865, cfg={'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.870, est=0.868, cfg={'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.871, est=0.863, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "Accuracy: 0.867 (0.007)\n",
      "Training Master: {'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.857 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.833, est=0.851, cfg={'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.839, est=0.851, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.852, est=0.847, cfg={'max_depth': 10, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.850, est=0.849, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.852, est=0.844, cfg={'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      "Accuracy: 0.845 (0.008)\n",
      "Training Master: {'max_depth': 10, 'max_features': 16, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.856 \n",
      "\n",
      "========================\n",
      "Starting data set 2...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.864, est=0.882, cfg={'max_depth': None, 'max_features': 4, 'n_estimators': 100}\n",
      ">acc=0.889, est=0.879, cfg={'max_depth': 100, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.896, est=0.880, cfg={'max_depth': None, 'max_features': 8, 'n_estimators': 100}\n",
      ">acc=0.902, est=0.881, cfg={'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      ">acc=0.893, est=0.878, cfg={'max_depth': 100, 'max_features': 6, 'n_estimators': 100}\n",
      "Accuracy: 0.889 (0.013)\n",
      "Training Master: {'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.898 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.889, est=0.880, cfg={'max_depth': None, 'max_features': 8, 'n_estimators': 100}\n",
      ">acc=0.876, est=0.889, cfg={'max_depth': None, 'max_features': 2, 'n_estimators': 100}\n",
      ">acc=0.898, est=0.879, cfg={'max_depth': 100, 'max_features': 4, 'n_estimators': 100}\n",
      ">acc=0.879, est=0.880, cfg={'max_depth': None, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.893, est=0.882, cfg={'max_depth': 100, 'max_features': 6, 'n_estimators': 100}\n",
      "Accuracy: 0.887 (0.008)\n",
      "Training Master: {'max_depth': 100, 'max_features': 4, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.891 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.897, est=0.886, cfg={'max_depth': None, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.875, est=0.880, cfg={'max_depth': 100, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.889, est=0.879, cfg={'max_depth': None, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.883, est=0.881, cfg={'max_depth': None, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.898, est=0.881, cfg={'max_depth': None, 'max_features': 8, 'n_estimators': 100}\n",
      "Accuracy: 0.888 (0.009)\n",
      "Training Master: {'max_depth': None, 'max_features': 8, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.906 \n",
      "\n",
      "========================\n",
      "Starting data set 3...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.800, est=0.808, cfg={'max_depth': None, 'max_features': 2, 'n_estimators': 100}\n",
      ">acc=0.822, est=0.810, cfg={'max_depth': None, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.818, est=0.805, cfg={'max_depth': 100, 'max_features': 4, 'n_estimators': 100}\n",
      ">acc=0.817, est=0.810, cfg={'max_depth': 100, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.789, est=0.817, cfg={'max_depth': None, 'max_features': 12, 'n_estimators': 100}\n",
      "Accuracy: 0.809 (0.013)\n",
      "Training Master: {'max_depth': None, 'max_features': 20, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.817 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.832, est=0.810, cfg={'max_depth': 100, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.803, est=0.818, cfg={'max_depth': 100, 'max_features': 16, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.823, est=0.807, cfg={'max_depth': None, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.794, est=0.814, cfg={'max_depth': None, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.813, est=0.809, cfg={'max_depth': None, 'max_features': 20, 'n_estimators': 100}\n",
      "Accuracy: 0.813 (0.014)\n",
      "Training Master: {'max_depth': 100, 'max_features': 16, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.823 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.801, est=0.797, cfg={'max_depth': None, 'max_features': 4, 'n_estimators': 100}\n",
      ">acc=0.802, est=0.803, cfg={'max_depth': None, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.817, est=0.797, cfg={'max_depth': None, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.802, est=0.805, cfg={'max_depth': None, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.787, est=0.801, cfg={'max_depth': 100, 'max_features': 4, 'n_estimators': 100}\n",
      "Accuracy: 0.802 (0.009)\n",
      "Training Master: {'max_depth': None, 'max_features': 20, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.823 \n",
      "\n",
      "Starting: GaussianNB\n",
      "~~~~~~~~~~~\n",
      "\n",
      "========================\n",
      "Starting data set 1...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.581, est=0.518, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.566, est=0.564, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.559, est=0.547, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.528, est=0.536, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.552, est=0.542, cfg={'var_smoothing': 1e-05}\n",
      "Accuracy: 0.557 (0.017)\n",
      "Training Master: {'var_smoothing': 1e-05}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.545 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.589, est=0.591, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.603, est=0.568, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.588, est=0.580, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.571, est=0.557, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.614, est=0.621, cfg={'var_smoothing': 1e-05}\n",
      "Accuracy: 0.593 (0.015)\n",
      "Training Master: {'var_smoothing': 1e-05}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.605 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.545, est=0.544, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.493, est=0.499, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.501, est=0.483, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.539, est=0.499, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.505, est=0.514, cfg={'var_smoothing': 1e-05}\n",
      "Accuracy: 0.517 (0.021)\n",
      "Training Master: {'var_smoothing': 1e-05}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.537 \n",
      "\n",
      "========================\n",
      "Starting data set 2...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.639, est=0.607, cfg={'var_smoothing': 1e-08}\n",
      ">acc=0.567, est=0.595, cfg={'var_smoothing': 1e-06}\n",
      ">acc=0.601, est=0.597, cfg={'var_smoothing': 1e-08}\n",
      ">acc=0.596, est=0.592, cfg={'var_smoothing': 1e-07}\n",
      ">acc=0.553, est=0.562, cfg={'var_smoothing': 1e-06}\n",
      "Accuracy: 0.591 (0.030)\n",
      "Training Master: {'var_smoothing': 1e-08}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.597 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.641, est=0.611, cfg={'var_smoothing': 1e-06}\n",
      ">acc=0.603, est=0.615, cfg={'var_smoothing': 1e-08}\n",
      ">acc=0.558, est=0.580, cfg={'var_smoothing': 1e-07}\n",
      ">acc=0.605, est=0.597, cfg={'var_smoothing': 1e-07}\n",
      ">acc=0.622, est=0.612, cfg={'var_smoothing': 1e-07}\n",
      "Accuracy: 0.606 (0.028)\n",
      "Training Master: {'var_smoothing': 1e-06}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.618 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.471, est=0.497, cfg={'var_smoothing': 1e-08}\n",
      ">acc=0.474, est=0.502, cfg={'var_smoothing': 1e-07}\n",
      ">acc=0.465, est=0.496, cfg={'var_smoothing': 1e-07}\n",
      ">acc=0.465, est=0.490, cfg={'var_smoothing': 1e-07}\n",
      ">acc=0.620, est=0.613, cfg={'var_smoothing': 1e-08}\n",
      "Accuracy: 0.499 (0.061)\n",
      "Training Master: {'var_smoothing': 1e-08}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.459 \n",
      "\n",
      "========================\n",
      "Starting data set 3...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.623, est=0.619, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.643, est=0.613, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.614, est=0.622, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.615, est=0.616, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.590, est=0.630, cfg={'var_smoothing': 1e-05}\n",
      "Accuracy: 0.617 (0.017)\n",
      "Training Master: {'var_smoothing': 1e-05}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.628 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.662, est=0.656, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.665, est=0.653, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.662, est=0.654, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.649, est=0.665, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.653, est=0.659, cfg={'var_smoothing': 1e-05}\n",
      "Accuracy: 0.658 (0.006)\n",
      "Training Master: {'var_smoothing': 1e-05}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.655 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.644, est=0.665, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.644, est=0.667, cfg={'var_smoothing': 1e-07}\n",
      ">acc=0.696, est=0.652, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.663, est=0.646, cfg={'var_smoothing': 1e-05}\n",
      ">acc=0.642, est=0.666, cfg={'var_smoothing': 1e-05}\n",
      "Accuracy: 0.658 (0.021)\n",
      "Training Master: {'var_smoothing': 1e-05}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.657 \n",
      "\n",
      "CPU times: user 7min 46s, sys: 7.49 s, total: 7min 53s\n",
      "Wall time: 32min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    {\n",
    "        'name': 'SVC',\n",
    "        'method': SVC,\n",
    "        'p_grid': {'C': [1,10,100,1000], 'gamma': [0.001,0.01,0.1,1.0]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'RandomForestClassifier',\n",
    "        'method': RandomForestClassifier,\n",
    "        'p_grid': {'n_estimators': [100], 'max_features': [1,2,4,6,8,12,16,20], 'max_depth': [10,100,None]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'GaussianNB',\n",
    "        'method': GaussianNB,\n",
    "        'p_grid': {'var_smoothing': [1.0e-5, 1.0e-6, 1.0e-7, 1.0e-8, 1.0e-9]}\n",
    "    }\n",
    "]\n",
    "\n",
    "# svm_grid = {'C': [1,10,100,1000], 'gamma': [0.001,0.01,0.1,1.0]}\n",
    "# nb_grid = {'var_smoothing': [1.0e-5, 1.0e-6, 1.0e-7, 1.0e-8, 1.0e-9]}\n",
    "# rf_grid = {'n_estimators': [100], 'max_features': [1,2,4,6,8,12,16,20], 'max_depth': [10,100,None]}\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "# svm = SVC(kernel='rbf')\n",
    "# nb = GaussianNB()\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print('Starting: {}'.format(classifier['name']))\n",
    "    print('~~~~~~~~~~~\\n')\n",
    "    clf = classifier['method']()\n",
    "    p_grid = classifier['p_grid']\n",
    "    \n",
    "    for d_i in range(len(all_data)):\n",
    "        data_set = all_data[d_i]\n",
    "        print('========================')\n",
    "        print('Starting data set {}...'.format(d_i+1))\n",
    "        print('========================')\n",
    "\n",
    "        for i in range(NUM_TRIALS):\n",
    "\n",
    "            training_set, testing_set = get_training_test_sets(data_set, DATA_SIZE, pred_col='y')\n",
    "\n",
    "            print('Trial {}...'.format(i+1))\n",
    "            trial_results = []\n",
    "            outer_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "            best_p = []\n",
    "            best_score = []\n",
    "\n",
    "            for tr_i, tst_i in outer_cv.split(training_set.X):\n",
    "                X_train, X_test = training_set.X[tr_i, :], training_set.X[tst_i, :]\n",
    "                y_train, y_test = training_set.y[tr_i], training_set.y[tst_i]\n",
    "\n",
    "                inner_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "\n",
    "\n",
    "\n",
    "                search = GridSearchCV(\n",
    "                    estimator=clf,\n",
    "                    param_grid=p_grid,\n",
    "                    cv=inner_cv,\n",
    "                    verbose=0,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    refit=True\n",
    "                )\n",
    "\n",
    "                result = search.fit(X_train, y_train)\n",
    "\n",
    "                model = result.best_estimator_\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                best_p.append(result.best_params_)\n",
    "                best_score.append(acc)\n",
    "                trial_results.append(acc)\n",
    "\n",
    "                print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "\n",
    "\n",
    "            print('Accuracy: %.3f (%.3f)' % (np.mean(trial_results), np.std(trial_results)))\n",
    "\n",
    "            run = best_score.index(max(best_score))\n",
    "            best_best_p = best_p[run]\n",
    "\n",
    "            oo_clf = classifier['method']()\n",
    "            print('Training Master: %s' % best_best_p)\n",
    "            oo_clf.set_params(**best_best_p)\n",
    "            oo_clf.fit(training_set.X,training_set.y)\n",
    "\n",
    "            print('Check Acc on Entire set')\n",
    "            y_pred = oo_clf.predict(testing_set.X)\n",
    "            acc = accuracy_score(testing_set.y, y_pred)\n",
    "            print('Final Acc = %.3f \\n' % acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
