{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables(at the top for quick access).\n",
    "NUM_TRIALS = 3\n",
    "FOLDS = 5\n",
    "DATA_SIZE = 500 #5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helpers\n",
    "class Set:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "# splits targets from data, then splits training from testing data\n",
    "def get_training_test_sets(data, training_size=1000, pred_col='y'):\n",
    "    data_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "    data_shuffled_y = pd.DataFrame(data_shuffled[pred_col])\n",
    "    data_shuffled_X = data_shuffled.drop(pred_col, 1)\n",
    "    tr_X = data_shuffled_X.iloc[:training_size, :].to_numpy()\n",
    "    tr_y = data_shuffled_y.iloc[:training_size, :].values.ravel()\n",
    "    tst_X = data_shuffled_X.iloc[training_size:, :].to_numpy()\n",
    "    tst_y = data_shuffled_y.iloc[training_size:, :].values.ravel()\n",
    "\n",
    "    training = Set(tr_X, tr_y)\n",
    "    testing = Set(tst_X, tst_y)\n",
    "    \n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(raw_data,\n",
    "               column_names=None,\n",
    "               binary_cols=None,\n",
    "               one_hot_cols=None,\n",
    "               continuous_cols=None,\n",
    "              ):\n",
    "    \n",
    "    final_data = raw_data\n",
    "    \n",
    "    if column_names is not None:\n",
    "        final_data.columns = column_names\n",
    "        \n",
    "        if binary_cols is not None:\n",
    "            for col in binary_cols:\n",
    "                if type(col) is tuple:\n",
    "                    match = col[1]\n",
    "                    ind = col[0]\n",
    "                else:\n",
    "                    match = final_data[col].unique()[0]\n",
    "                    ind = col\n",
    "                final_data[ind] = (final_data[ind] != match).astype(int)\n",
    "                \n",
    "        if one_hot_cols is not None:\n",
    "            final_data = pd.get_dummies(final_data, columns=one_hot_cols)\n",
    "            \n",
    "        if final_data.isna().values.any():\n",
    "            print('Warning!: missing data')\n",
    "            \n",
    "        if continuous_cols is not None:\n",
    "            col_names = final_data.columns\n",
    "            mask = np.isin(col_names, continuous_cols, invert=True)\n",
    "            not_continuous = col_names[mask]\n",
    "            \n",
    "            reordered_cols = np.concatenate((continuous_cols, not_continuous))\n",
    "            final_data = final_data[reordered_cols]\n",
    "            \n",
    "            # Normalize\n",
    "            ct = ColumnTransformer([\n",
    "                ('continuous', StandardScaler(), continuous_cols)\n",
    "                \n",
    "            ], remainder='passthrough')\n",
    "            \n",
    "            scaled = ct.fit_transform(final_data)\n",
    "            final_data = pd.DataFrame(scaled, columns=reordered_cols)\n",
    "            \n",
    "    else:\n",
    "        print('No columns names, returning raw data.')\n",
    "        \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set\n",
    "adults_raw = pd.read_csv('data/adults/adult.data', header=None)\n",
    "\n",
    "eye_arff = arff.loadarff('data/eeg_eye/EEG_Eye_State.arff')\n",
    "eyes_raw = pd.DataFrame(eye_arff[0])\n",
    "\n",
    "covertype_raw = pd.read_csv('data/covertype/covtype.data', header=None)\n",
    "\n",
    "\n",
    "\n",
    "adult_process_params = {\n",
    "    'column_names': ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'y'],\n",
    "    'binary_cols': ['sex', 'y'],\n",
    "    'one_hot_cols': ['workclass', 'education', 'marital_status', 'occupation','relationship', 'race', 'native_country'],\n",
    "    'continuous_cols': ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'],\n",
    "}\n",
    "\n",
    "eyes_process_params = {\n",
    "    'column_names': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4', 'y'],\n",
    "    'binary_cols': ['y'],\n",
    "    'continuous_cols': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "}\n",
    "\n",
    "# [('y', 2)], # lodgepole pine\n",
    "covtyp_process_params = {\n",
    "    'column_names': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type0', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'y'],\n",
    "    'binary_cols': [('y', 2)],\n",
    "    'continuous_cols': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'],\n",
    "}\n",
    "# Cover Type column names can be rebuilt with the code below if need be.\n",
    "# cov_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
    "# Wilderness_Area = ['Wilderness_Area_{}'.format(i) for i in range(4)]\n",
    "# Soil_Type = ['Soil_Type{}'.format(i) for i in range(40)]\n",
    "# cov_cols = np.concatenate((cov_cols, Wilderness_Area, Soil_Type, ['y']))\n",
    "# covertype_raw.columns = cov_cols\n",
    "\n",
    "adults = clean_data(adults_raw, **adult_process_params)\n",
    "eyes = clean_data(eyes_raw, **eyes_process_params)\n",
    "pines = clean_data(covertype_raw, **covtyp_process_params)\n",
    "# print(adults.head())\n",
    "# print(eyes.head())\n",
    "# print(pines.head())\n",
    "\n",
    "all_data = [adults, eyes, pines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Starting data set 1...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.890, est=0.865, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.820, est=0.868, cfg={'max_depth': None, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.860, est=0.875, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.890, est=0.870, cfg={'max_depth': 100, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.880, est=0.867, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "Accuracy: 0.868 (0.026)\n",
      "Training Master: {'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.844 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.820, est=0.838, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.740, est=0.840, cfg={'max_depth': 10, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.840, est=0.830, cfg={'max_depth': None, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.830, est=0.838, cfg={'max_depth': 100, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.880, est=0.812, cfg={'max_depth': None, 'max_features': 12, 'n_estimators': 100}\n",
      "Accuracy: 0.822 (0.046)\n",
      "Training Master: {'max_depth': None, 'max_features': 12, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.834 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.880, est=0.823, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.810, est=0.850, cfg={'max_depth': 10, 'max_features': 8, 'n_estimators': 100}\n",
      ">acc=0.820, est=0.843, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.840, est=0.847, cfg={'max_depth': 10, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.800, est=0.852, cfg={'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      "Accuracy: 0.830 (0.028)\n",
      "Training Master: {'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.846 \n",
      "\n",
      "========================\n",
      "Starting data set 2...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.760, est=0.765, cfg={'max_depth': None, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.720, est=0.750, cfg={'max_depth': 100, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.770, est=0.732, cfg={'max_depth': 100, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.740, est=0.745, cfg={'max_depth': 100, 'max_features': 1, 'n_estimators': 100}\n",
      ">acc=0.770, est=0.728, cfg={'max_depth': 10, 'max_features': 8, 'n_estimators': 100}\n",
      "Accuracy: 0.752 (0.019)\n",
      "Training Master: {'max_depth': 100, 'max_features': 12, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.778 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.820, est=0.767, cfg={'max_depth': 100, 'max_features': 2, 'n_estimators': 100}\n",
      ">acc=0.800, est=0.802, cfg={'max_depth': None, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.790, est=0.812, cfg={'max_depth': 10, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.820, est=0.787, cfg={'max_depth': 100, 'max_features': 1, 'n_estimators': 100}\n",
      ">acc=0.770, est=0.792, cfg={'max_depth': 100, 'max_features': 6, 'n_estimators': 100}\n",
      "Accuracy: 0.800 (0.019)\n",
      "Training Master: {'max_depth': 100, 'max_features': 2, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.763 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.730, est=0.722, cfg={'max_depth': None, 'max_features': 1, 'n_estimators': 100}\n",
      ">acc=0.760, est=0.732, cfg={'max_depth': None, 'max_features': 4, 'n_estimators': 100}\n",
      ">acc=0.790, est=0.730, cfg={'max_depth': None, 'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.700, est=0.750, cfg={'max_depth': None, 'max_features': 2, 'n_estimators': 100}\n",
      ">acc=0.750, est=0.725, cfg={'max_depth': None, 'max_features': 2, 'n_estimators': 100}\n",
      "Accuracy: 0.746 (0.030)\n",
      "Training Master: {'max_depth': None, 'max_features': 6, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.787 \n",
      "\n",
      "========================\n",
      "Starting data set 3...\n",
      "========================\n",
      "Trial 1...\n",
      ">acc=0.770, est=0.755, cfg={'max_depth': 10, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.670, est=0.770, cfg={'max_depth': 100, 'max_features': 1, 'n_estimators': 100}\n",
      ">acc=0.700, est=0.780, cfg={'max_depth': 10, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.720, est=0.768, cfg={'max_depth': None, 'max_features': 8, 'n_estimators': 100}\n",
      ">acc=0.860, est=0.743, cfg={'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      "Accuracy: 0.744 (0.067)\n",
      "Training Master: {'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.749 \n",
      "\n",
      "Trial 2...\n",
      ">acc=0.760, est=0.775, cfg={'max_depth': None, 'max_features': 8, 'n_estimators': 100}\n",
      ">acc=0.710, est=0.785, cfg={'max_depth': 100, 'max_features': 20, 'n_estimators': 100}\n",
      ">acc=0.730, est=0.778, cfg={'max_depth': None, 'max_features': 1, 'n_estimators': 100}\n",
      ">acc=0.750, est=0.765, cfg={'max_depth': 10, 'max_features': 12, 'n_estimators': 100}\n",
      ">acc=0.790, est=0.755, cfg={'max_depth': 10, 'max_features': 4, 'n_estimators': 100}\n",
      "Accuracy: 0.748 (0.027)\n",
      "Training Master: {'max_depth': 10, 'max_features': 4, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.748 \n",
      "\n",
      "Trial 3...\n",
      ">acc=0.760, est=0.705, cfg={'max_depth': 100, 'max_features': 1, 'n_estimators': 100}\n",
      ">acc=0.700, est=0.730, cfg={'max_depth': 100, 'max_features': 16, 'n_estimators': 100}\n",
      ">acc=0.750, est=0.735, cfg={'max_depth': 10, 'max_features': 2, 'n_estimators': 100}\n",
      ">acc=0.710, est=0.690, cfg={'max_depth': None, 'max_features': 1, 'n_estimators': 100}\n",
      ">acc=0.710, est=0.718, cfg={'max_depth': 100, 'max_features': 4, 'n_estimators': 100}\n",
      "Accuracy: 0.726 (0.024)\n",
      "Training Master: {'max_depth': 100, 'max_features': 1, 'n_estimators': 100}\n",
      "Check Acc on Entire set\n",
      "Final Acc = 0.725 \n",
      "\n",
      "CPU times: user 34.7 s, sys: 2.5 s, total: 37.2 s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    {\n",
    "        'name': 'SVC',\n",
    "        'method': SVC,\n",
    "        'p_grid': {'C': [1,10,100,1000], 'gamma': [0.001,0.01,0.1,1.0]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'RandomForestClassifier',\n",
    "        'method': RandomForestClassifier,\n",
    "        'p_grid': {'n_estimators': [100], 'max_features': [1,2,4,6,8,12,16,20], 'max_depth': [10,100,None]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'GaussianNB',\n",
    "        'method': GaussianNB,\n",
    "        'p_grid': {'var_smoothing': [1.0e-5, 1.0e-6, 1.0e-7, 1.0e-8, 1.0e-9]}\n",
    "    }\n",
    "]\n",
    "\n",
    "# svm_grid = {'C': [1,10,100,1000], 'gamma': [0.001,0.01,0.1,1.0]}\n",
    "# nb_grid = {'var_smoothing': [1.0e-5, 1.0e-6, 1.0e-7, 1.0e-8, 1.0e-9]}\n",
    "# rf_grid = {'n_estimators': [100], 'max_features': [1,2,4,6,8,12,16,20], 'max_depth': [10,100,None]}\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "# svm = SVC(kernel='rbf')\n",
    "# nb = GaussianNB()\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print('Starting: {}'.format(classifier['name']))\n",
    "    print('~~~~~~~~~~~\\n')\n",
    "    clf = classifier['method']()\n",
    "    p_grid = classifier['p_grid']\n",
    "    \n",
    "    for d_i in range(len(all_data)):\n",
    "        data_set = all_data[d_i]\n",
    "        print('========================')\n",
    "        print('Starting data set {}...'.format(d_i+1))\n",
    "        print('========================')\n",
    "\n",
    "        for i in range(NUM_TRIALS):\n",
    "\n",
    "            training_set, testing_set = get_training_test_sets(data_set, DATA_SIZE, pred_col='y')\n",
    "\n",
    "            print('Trial {}...'.format(i+1))\n",
    "            trial_results = []\n",
    "            outer_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "            best_p = []\n",
    "            best_score = []\n",
    "\n",
    "            for tr_i, tst_i in outer_cv.split(training_set.X):\n",
    "                X_train, X_test = training_set.X[tr_i, :], training_set.X[tst_i, :]\n",
    "                y_train, y_test = training_set.y[tr_i], training_set.y[tst_i]\n",
    "\n",
    "                inner_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "\n",
    "\n",
    "\n",
    "                search = GridSearchCV(\n",
    "                    estimator=clf,\n",
    "                    param_grid=p_grid,\n",
    "                    cv=inner_cv,\n",
    "                    verbose=0,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    refit=True\n",
    "                )\n",
    "\n",
    "                result = search.fit(X_train, y_train)\n",
    "\n",
    "                model = result.best_estimator_\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                best_p.append(result.best_params_)\n",
    "                best_score.append(acc)\n",
    "                trial_results.append(acc)\n",
    "\n",
    "                print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "\n",
    "\n",
    "            print('Accuracy: %.3f (%.3f)' % (np.mean(trial_results), np.std(trial_results)))\n",
    "\n",
    "            run = best_score.index(max(best_score))\n",
    "            best_best_p = best_p[run]\n",
    "\n",
    "            oo_clf = classifier['method']()\n",
    "            print('Training Master: %s' % best_best_p)\n",
    "            oo_clf.set_params(**best_best_p)\n",
    "            oo_clf.fit(training_set.X,training_set.y)\n",
    "\n",
    "            print('Check Acc on Entire set')\n",
    "            y_pred = oo_clf.predict(testing_set.X)\n",
    "            acc = accuracy_score(testing_set.y, y_pred)\n",
    "            print('Final Acc = %.3f \\n' % acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
