{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables(at the top for quick access).\n",
    "NUM_TRIALS = 3\n",
    "FOLDS = 5\n",
    "DATA_SIZE = 5000 #5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helpers\n",
    "class Set:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "# splits targets from data, then splits training from testing data\n",
    "def get_training_test_sets(data, training_size=1000, pred_col='y'):\n",
    "    data_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "    data_shuffled_y = pd.DataFrame(data_shuffled[pred_col])\n",
    "    data_shuffled_X = data_shuffled.drop(pred_col, 1)\n",
    "    tr_X = data_shuffled_X.iloc[:training_size, :].to_numpy()\n",
    "    tr_y = data_shuffled_y.iloc[:training_size, :].values.ravel()\n",
    "    tst_X = data_shuffled_X.iloc[training_size:, :].to_numpy()\n",
    "    tst_y = data_shuffled_y.iloc[training_size:, :].values.ravel()\n",
    "\n",
    "    training = Set(tr_X, tr_y)\n",
    "    testing = Set(tst_X, tst_y)\n",
    "    \n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(raw_data,\n",
    "               column_names=None,\n",
    "               binary_cols=None,\n",
    "               one_hot_cols=None,\n",
    "               continuous_cols=None,\n",
    "              ):\n",
    "    \n",
    "    final_data = raw_data\n",
    "    \n",
    "    if column_names is not None:\n",
    "        final_data.columns = column_names\n",
    "        \n",
    "        if binary_cols is not None:\n",
    "            for col in binary_cols:\n",
    "                if type(col) is tuple:\n",
    "                    match = col[1]\n",
    "                    ind = col[0]\n",
    "                else:\n",
    "                    match = final_data[col].unique()[0]\n",
    "                    ind = col\n",
    "                final_data[ind] = (final_data[ind] != match).astype(int)\n",
    "                \n",
    "        if one_hot_cols is not None:\n",
    "            final_data = pd.get_dummies(final_data, columns=one_hot_cols)\n",
    "            \n",
    "        if final_data.isna().values.any():\n",
    "            print('Warning!: missing data')\n",
    "            \n",
    "        if continuous_cols is not None:\n",
    "            col_names = final_data.columns\n",
    "            mask = np.isin(col_names, continuous_cols, invert=True)\n",
    "            not_continuous = col_names[mask]\n",
    "            \n",
    "            reordered_cols = np.concatenate((continuous_cols, not_continuous))\n",
    "            final_data = final_data[reordered_cols]\n",
    "            \n",
    "            # Normalize\n",
    "            ct = ColumnTransformer([\n",
    "                ('continuous', StandardScaler(), continuous_cols)\n",
    "                \n",
    "            ], remainder='passthrough')\n",
    "            \n",
    "            scaled = ct.fit_transform(final_data)\n",
    "            final_data = pd.DataFrame(scaled, columns=reordered_cols)\n",
    "            \n",
    "    else:\n",
    "        print('No columns names, returning raw data.')\n",
    "        \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set\n",
    "adults_raw = pd.read_csv('data/adults/adult.data', header=None)\n",
    "\n",
    "eye_arff = arff.loadarff('data/eeg_eye/EEG_Eye_State.arff')\n",
    "eyes_raw = pd.DataFrame(eye_arff[0])\n",
    "\n",
    "covertype_raw = pd.read_csv('data/covertype/covtype.data', header=None)\n",
    "\n",
    "\n",
    "\n",
    "adult_process_params = {\n",
    "    'column_names': ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'y'],\n",
    "    'binary_cols': ['sex', 'y'],\n",
    "    'one_hot_cols': ['workclass', 'education', 'marital_status', 'occupation','relationship', 'race', 'native_country'],\n",
    "    'continuous_cols': ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'],\n",
    "}\n",
    "\n",
    "eyes_process_params = {\n",
    "    'column_names': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4', 'y'],\n",
    "    'binary_cols': ['y'],\n",
    "    'continuous_cols': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "}\n",
    "\n",
    "# [('y', 2)], # lodgepole pine\n",
    "covtyp_process_params = {\n",
    "    'column_names': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type0', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'y'],\n",
    "    'binary_cols': [('y', 2)],\n",
    "    'continuous_cols': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'],\n",
    "}\n",
    "# Cover Type column names can be rebuilt with the code below if need be.\n",
    "# cov_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
    "# Wilderness_Area = ['Wilderness_Area_{}'.format(i) for i in range(4)]\n",
    "# Soil_Type = ['Soil_Type{}'.format(i) for i in range(40)]\n",
    "# cov_cols = np.concatenate((cov_cols, Wilderness_Area, Soil_Type, ['y']))\n",
    "# covertype_raw.columns = cov_cols\n",
    "\n",
    "adults = clean_data(adults_raw, **adult_process_params)\n",
    "eyes = clean_data(eyes_raw, **eyes_process_params)\n",
    "pines = clean_data(covertype_raw, **covtyp_process_params)\n",
    "# print(adults.head())\n",
    "# print(eyes.head())\n",
    "# print(pines.head())\n",
    "\n",
    "all_data = [adults, eyes, pines]\n",
    "data_names = ['ADULTS', 'EEG_EYE', 'COV_TYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNM06 Table 1\n",
    "print('PROBLEM, #ATTR, TRAIN SIZE, TEST SIZE, %POZ')\n",
    "for i in range(len(all_data)):\n",
    "    d = all_data[i]\n",
    "    name = data_names[i]\n",
    "    features = (len(d.columns)-1)\n",
    "    entries = len(d)\n",
    "    pos = 100 * d['y'].sum() / entries\n",
    "    print('{}, {}, 5000, {}, {}%'.format(name, features, entries, int(pos)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "classifiers = [\n",
    "    {\n",
    "        'name': 'SVM',\n",
    "        'method': SVC,\n",
    "        'p_grid': {'C': [1,10,100,1000], 'gamma': [0.001,0.01,0.1,1.0]},\n",
    "        'test_acc': list()\n",
    "    },\n",
    "    {\n",
    "        'name': 'RF',\n",
    "        'method': RandomForestClassifier,\n",
    "        'p_grid': {'n_estimators': [100], 'max_features': ['sqrt',2,5,8,16,18,20,36], 'max_depth': [10,100,None]},\n",
    "        'test_acc': list()\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'name': 'gNB',\n",
    "        'method': GaussianNB,\n",
    "        'p_grid': {'var_smoothing': [1.0e-5, 1.0e-6, 1.0e-7, 1.0e-8, 1.0e-9]},\n",
    "        'test_acc': list()\n",
    "    }\n",
    "]\n",
    "\n",
    "gridcvs = {\n",
    "    'SVM': {},\n",
    "    'RF': {},\n",
    "    'gNB': {},\n",
    "}\n",
    "\n",
    "tst_models = {\n",
    "    'SVM': {},\n",
    "    'RF': {},\n",
    "    'gNB': {},\n",
    "}\n",
    "\n",
    "dbs_acc = {}\n",
    "for name in data_names:\n",
    "    dbs_acc[name] = []\n",
    "\n",
    "tbl_2 = []\n",
    "tbl_3 = []\n",
    "    \n",
    "for classifier in classifiers:\n",
    "    print('========================')\n",
    "    print('Starting: {}'.format(classifier['name']))\n",
    "    print('========================\\n')\n",
    "    \n",
    "    clf = classifier['method']()\n",
    "    p_grid = classifier['p_grid']\n",
    "    \n",
    "    for d_i in range(len(all_data)):\n",
    "        data_set = all_data[d_i]\n",
    "        name = data_names[d_i]\n",
    "        \n",
    "        gridcvs[classifier['name']][name] = []\n",
    "        tst_models[classifier['name']][name] = []\n",
    "        \n",
    "        print('  Starting data set {}...'.format(name))\n",
    "        print('  ~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "        test_set_scores = []\n",
    "        for i in range(NUM_TRIALS):\n",
    "\n",
    "            training_set, testing_set = get_training_test_sets(data_set, DATA_SIZE, pred_col='y')\n",
    "\n",
    "            print('  Trial {}...'.format(i+1))\n",
    "            trial_results = []\n",
    "            outer_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "            best_p = []\n",
    "            best_score = []\n",
    "\n",
    "            print('    Training Results (outer)')\n",
    "            for tr_i, tst_i in outer_cv.split(training_set.X):\n",
    "                X_train, X_test = training_set.X[tr_i, :], training_set.X[tst_i, :]\n",
    "                y_train, y_test = training_set.y[tr_i], training_set.y[tst_i]\n",
    "\n",
    "                inner_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "\n",
    "\n",
    "\n",
    "                search = GridSearchCV(\n",
    "                    estimator=clf,\n",
    "                    param_grid=p_grid,\n",
    "                    cv=inner_cv,\n",
    "                    verbose=0,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    refit=True\n",
    "                )\n",
    "\n",
    "                result = search.fit(X_train, y_train)\n",
    "                \n",
    "                gridcvs[classifier['name']][name].append(result)\n",
    "                \n",
    "                model = result.best_estimator_\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                best_p.append(result.best_params_)\n",
    "                best_score.append(acc)\n",
    "                trial_results.append(acc)\n",
    "\n",
    "                print('    >acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "\n",
    "\n",
    "            \n",
    "            print('    Overall Training Acc: %.3f std(%.3f)' % (np.mean(trial_results), np.std(trial_results)))\n",
    "\n",
    "            run = best_score.index(max(best_score))\n",
    "            best_best_p = best_p[run]\n",
    "\n",
    "            oo_clf = classifier['method']()\n",
    "            print('  Training Master for test: %s' % best_best_p)\n",
    "            oo_clf.set_params(**best_best_p)\n",
    "            outer_results = oo_clf.fit(training_set.X,training_set.y)\n",
    "            \n",
    "            tst_models[classifier['name']][name].append(outer_results)\n",
    "            \n",
    "            y_pred = oo_clf.predict(testing_set.X)\n",
    "            acc = accuracy_score(testing_set.y, y_pred)\n",
    "            classifier['test_acc'].append(acc)\n",
    "            dbs_acc[name].append(acc)\n",
    "            test_set_scores.append(acc)\n",
    "            print('  Test_Set Acc = %.3f \\n' % acc)\n",
    "            \n",
    "        tbl_2.append((classifier['name'], np.mean(test_set_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 2\n",
    "print('MODEL, ACC')\n",
    "for name, val in tbl_2:\n",
    "    print('{}, {:.3f}'.format(name, val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 3\n",
    "quick_stack = {\n",
    "    'SVM': [],\n",
    "    'RF': [],\n",
    "    'gNB': [],\n",
    "}\n",
    "\n",
    "print('MODEL, ADULTS, EEG_EYE, COV_TYPE, MEAN')\n",
    "for el in tbl_2:\n",
    "    quick_stack[el[0]].append(el[1])\n",
    "    \n",
    "for key, values in quick_stack.items():\n",
    "    val_str = ['{:.3f}'.format(x) for x in values]\n",
    "    print('{}, {}, {:.3f}'.format(key, \", \".join(val_str), np.mean(values)))\n",
    "#     for val in values:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-statistic and p-value table\n",
    "test_perm = combinations(classifiers, 2)\n",
    "print('Algo_1, Algo_2, statistic, p-value')\n",
    "for pair in list(test_perm):\n",
    "    a = pair[0]\n",
    "    b = pair[1]\n",
    "    \n",
    "    t,p = stats.ttest_ind(a['test_acc'],b['test_acc'], equal_var=False)\n",
    "    print('{}, {}, {:.3f}, {:.3g}'.format(a['name'], b['name'], t, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keepin' the  books\n",
    "\n",
    "# Models actually used for test set predictions\n",
    "# print(type(tst_models['SVM']['ADULTS'][0]))\n",
    "filename = 'all_test_models.sav'\n",
    "pickle.dump(tst_models, open(filename, 'wb'))\n",
    "\n",
    "# load the tst_models\n",
    "# tst_models = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# GridSearchCV results\n",
    "f = open(\"grid_results.txt\", \"w\")\n",
    "for key, clf_arr in gridcvs.items():\n",
    "    for d_name, d_set in clf_arr.items():\n",
    "        for fold in d_set:\n",
    "#             print(fold.cv_results_)\n",
    "            f.write('{} - {}:\\n-------------\\n'.format(key,d_name))\n",
    "            f.write(str(fold.cv_results_))\n",
    "            f.write(\"\\n\\n===================================\\n\")\n",
    "            f.write(\"===================================\\n\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
