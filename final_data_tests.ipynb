{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables(at the top for quick access).\n",
    "NUM_TRIALS = 3\n",
    "FOLDS = 5\n",
    "DATA_SIZE = 5000 #5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helpers\n",
    "class Set:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "# splits targets from data, then splits training from testing data\n",
    "def get_training_test_sets(data, training_size=1000, pred_col='y'):\n",
    "    data_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
    "    data_shuffled_y = pd.DataFrame(data_shuffled[pred_col])\n",
    "    data_shuffled_X = data_shuffled.drop(pred_col, 1)\n",
    "    tr_X = data_shuffled_X.iloc[:training_size, :].to_numpy()\n",
    "    tr_y = data_shuffled_y.iloc[:training_size, :].values.ravel()\n",
    "    tst_X = data_shuffled_X.iloc[training_size:, :].to_numpy()\n",
    "    tst_y = data_shuffled_y.iloc[training_size:, :].values.ravel()\n",
    "\n",
    "    training = Set(tr_X, tr_y)\n",
    "    testing = Set(tst_X, tst_y)\n",
    "    \n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(raw_data,\n",
    "               column_names=None,\n",
    "               binary_cols=None,\n",
    "               one_hot_cols=None,\n",
    "               continuous_cols=None,\n",
    "              ):\n",
    "    \n",
    "    final_data = raw_data\n",
    "    \n",
    "    if column_names is not None:\n",
    "        final_data.columns = column_names\n",
    "        \n",
    "        if binary_cols is not None:\n",
    "            for col in binary_cols:\n",
    "                if type(col) is tuple:\n",
    "                    match = col[1]\n",
    "                    ind = col[0]\n",
    "                else:\n",
    "                    match = final_data[col].unique()[0]\n",
    "                    ind = col\n",
    "                final_data[ind] = (final_data[ind] != match).astype(int)\n",
    "                \n",
    "        if one_hot_cols is not None:\n",
    "            final_data = pd.get_dummies(final_data, columns=one_hot_cols)\n",
    "            \n",
    "        if final_data.isna().values.any():\n",
    "            print('Warning!: missing data')\n",
    "            \n",
    "        if continuous_cols is not None:\n",
    "            col_names = final_data.columns\n",
    "            mask = np.isin(col_names, continuous_cols, invert=True)\n",
    "            not_continuous = col_names[mask]\n",
    "            \n",
    "            reordered_cols = np.concatenate((continuous_cols, not_continuous))\n",
    "            final_data = final_data[reordered_cols]\n",
    "            \n",
    "            # Normalize\n",
    "            ct = ColumnTransformer([\n",
    "                ('continuous', StandardScaler(), continuous_cols)\n",
    "                \n",
    "            ], remainder='passthrough')\n",
    "            \n",
    "            scaled = ct.fit_transform(final_data)\n",
    "            final_data = pd.DataFrame(scaled, columns=reordered_cols)\n",
    "            \n",
    "    else:\n",
    "        print('No columns names, returning raw data.')\n",
    "        \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set\n",
    "adults_raw = pd.read_csv('data/adults/adult.data', header=None)\n",
    "\n",
    "eye_arff = arff.loadarff('data/eeg_eye/EEG_Eye_State.arff')\n",
    "eyes_raw = pd.DataFrame(eye_arff[0])\n",
    "\n",
    "covertype_raw = pd.read_csv('data/covertype/covtype.data', header=None)\n",
    "\n",
    "\n",
    "\n",
    "adult_process_params = {\n",
    "    'column_names': ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'y'],\n",
    "    'binary_cols': ['sex', 'y'],\n",
    "    'one_hot_cols': ['workclass', 'education', 'marital_status', 'occupation','relationship', 'race', 'native_country'],\n",
    "    'continuous_cols': ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'],\n",
    "}\n",
    "\n",
    "eyes_process_params = {\n",
    "    'column_names': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4', 'y'],\n",
    "    'binary_cols': ['y'],\n",
    "    'continuous_cols': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "}\n",
    "\n",
    "# [('y', 2)], # lodgepole pine\n",
    "covtyp_process_params = {\n",
    "    'column_names': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type0', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'y'],\n",
    "    'binary_cols': [('y', 2)],\n",
    "    'continuous_cols': ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'],\n",
    "}\n",
    "# Cover Type column names can be rebuilt with the code below if need be.\n",
    "# cov_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
    "# Wilderness_Area = ['Wilderness_Area_{}'.format(i) for i in range(4)]\n",
    "# Soil_Type = ['Soil_Type{}'.format(i) for i in range(40)]\n",
    "# cov_cols = np.concatenate((cov_cols, Wilderness_Area, Soil_Type, ['y']))\n",
    "# covertype_raw.columns = cov_cols\n",
    "\n",
    "adults = clean_data(adults_raw, **adult_process_params)\n",
    "eyes = clean_data(eyes_raw, **eyes_process_params)\n",
    "pines = clean_data(covertype_raw, **covtyp_process_params)\n",
    "# print(adults.head())\n",
    "# print(eyes.head())\n",
    "# print(pines.head())\n",
    "\n",
    "all_data = [adults, eyes, pines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.666666666666664\n",
      "4.666666666666667\n",
      "18.0\n"
     ]
    }
   ],
   "source": [
    "# Targeted max_features for RandomForest n_feature/3\n",
    "for d in all_data:\n",
    "    print((len(d.columns)-1)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Starting: SVC\n",
      "========================\n",
      "\n",
      "  Starting data set 1...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "  >acc=0.853, est=0.855, cfg={'C': 10, 'gamma': 0.01}\n",
      "  >acc=0.845, est=0.861, cfg={'C': 1000, 'gamma': 0.001}\n",
      "  >acc=0.873, est=0.854, cfg={'C': 100, 'gamma': 0.001}\n",
      "  >acc=0.840, est=0.861, cfg={'C': 1, 'gamma': 0.1}\n",
      "  >acc=0.861, est=0.858, cfg={'C': 10, 'gamma': 0.01}\n",
      "  Accuracy: 0.854 (0.012)\n",
      "  Training Master: {'C': 100, 'gamma': 0.001}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.851 \n",
      "\n",
      "  Trial 2...\n",
      "  >acc=0.852, est=0.854, cfg={'C': 10, 'gamma': 0.01}\n",
      "  >acc=0.860, est=0.848, cfg={'C': 1000, 'gamma': 0.001}\n",
      "  >acc=0.851, est=0.853, cfg={'C': 1000, 'gamma': 0.001}\n",
      "  >acc=0.849, est=0.853, cfg={'C': 100, 'gamma': 0.001}\n",
      "  >acc=0.850, est=0.856, cfg={'C': 10, 'gamma': 0.01}\n",
      "  Accuracy: 0.852 (0.004)\n",
      "  Training Master: {'C': 1000, 'gamma': 0.001}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.851 \n",
      "\n",
      "  Trial 3...\n",
      "  >acc=0.849, est=0.855, cfg={'C': 1000, 'gamma': 0.001}\n",
      "  >acc=0.874, est=0.848, cfg={'C': 1, 'gamma': 0.1}\n",
      "  >acc=0.841, est=0.853, cfg={'C': 100, 'gamma': 0.01}\n",
      "  >acc=0.849, est=0.852, cfg={'C': 100, 'gamma': 0.001}\n",
      "  >acc=0.848, est=0.851, cfg={'C': 100, 'gamma': 0.01}\n",
      "  Accuracy: 0.852 (0.011)\n",
      "  Training Master: {'C': 1, 'gamma': 0.1}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.854 \n",
      "\n",
      "  Starting data set 2...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "  >acc=0.822, est=0.826, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  >acc=0.838, est=0.814, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  >acc=0.826, est=0.817, cfg={'C': 100, 'gamma': 1.0}\n",
      "  >acc=0.829, est=0.819, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  >acc=0.835, est=0.820, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  Accuracy: 0.830 (0.006)\n",
      "  Training Master: {'C': 1000, 'gamma': 1.0}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.842 \n",
      "\n",
      "  Trial 2...\n",
      "  >acc=0.831, est=0.816, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  >acc=0.848, est=0.808, cfg={'C': 100, 'gamma': 1.0}\n",
      "  >acc=0.809, est=0.812, cfg={'C': 100, 'gamma': 1.0}\n",
      "  >acc=0.844, est=0.818, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  >acc=0.819, est=0.819, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  Accuracy: 0.830 (0.015)\n",
      "  Training Master: {'C': 100, 'gamma': 1.0}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.842 \n",
      "\n",
      "  Trial 3...\n",
      "  >acc=0.862, est=0.813, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  >acc=0.832, est=0.828, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  >acc=0.826, est=0.823, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  >acc=0.835, est=0.828, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  >acc=0.830, est=0.820, cfg={'C': 1000, 'gamma': 1.0}\n",
      "  Accuracy: 0.837 (0.013)\n",
      "  Training Master: {'C': 1000, 'gamma': 1.0}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.853 \n",
      "\n",
      "  Starting data set 3...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "  >acc=0.805, est=0.796, cfg={'C': 10, 'gamma': 0.1}\n",
      "  >acc=0.784, est=0.800, cfg={'C': 10, 'gamma': 0.1}\n",
      "  >acc=0.811, est=0.799, cfg={'C': 10, 'gamma': 0.1}\n",
      "  >acc=0.797, est=0.798, cfg={'C': 10, 'gamma': 0.1}\n",
      "  >acc=0.804, est=0.794, cfg={'C': 10, 'gamma': 0.1}\n",
      "  Accuracy: 0.800 (0.009)\n",
      "  Training Master: {'C': 10, 'gamma': 0.1}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.802 \n",
      "\n",
      "  Trial 2...\n",
      "  >acc=0.794, est=0.801, cfg={'C': 10, 'gamma': 0.1}\n",
      "  >acc=0.796, est=0.795, cfg={'C': 10, 'gamma': 0.1}\n",
      "  >acc=0.793, est=0.808, cfg={'C': 10, 'gamma': 0.1}\n",
      "  >acc=0.815, est=0.799, cfg={'C': 10, 'gamma': 0.1}\n",
      "  >acc=0.797, est=0.789, cfg={'C': 100, 'gamma': 0.01}\n",
      "  Accuracy: 0.799 (0.008)\n",
      "  Training Master: {'C': 10, 'gamma': 0.1}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.804 \n",
      "\n",
      "  Trial 3...\n",
      "  >acc=0.802, est=0.787, cfg={'C': 10, 'gamma': 0.1}\n",
      "  >acc=0.789, est=0.790, cfg={'C': 1000, 'gamma': 0.01}\n",
      "  >acc=0.786, est=0.790, cfg={'C': 1000, 'gamma': 0.01}\n",
      "  >acc=0.785, est=0.784, cfg={'C': 1, 'gamma': 0.1}\n",
      "  >acc=0.796, est=0.784, cfg={'C': 10, 'gamma': 0.1}\n",
      "  Accuracy: 0.792 (0.006)\n",
      "  Training Master: {'C': 10, 'gamma': 0.1}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.807 \n",
      "\n",
      "========================\n",
      "Starting: RandomForestClassifier\n",
      "========================\n",
      "\n",
      "  Starting data set 1...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "  >acc=0.861, est=0.852, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  >acc=0.853, est=0.848, cfg={'max_depth': 10, 'max_features': 18, 'n_estimators': 100}\n",
      "  >acc=0.854, est=0.849, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  >acc=0.825, est=0.853, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  >acc=0.867, est=0.850, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  Accuracy: 0.852 (0.014)\n",
      "  Training Master: {'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.857 \n",
      "\n",
      "  Trial 2...\n",
      "  >acc=0.862, est=0.852, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  >acc=0.847, est=0.855, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "  >acc=0.861, est=0.851, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  >acc=0.852, est=0.852, cfg={'max_depth': 100, 'max_features': 20, 'n_estimators': 100}\n",
      "  >acc=0.845, est=0.853, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  Accuracy: 0.853 (0.007)\n",
      "  Training Master: {'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.859 \n",
      "\n",
      "  Trial 3...\n",
      "  >acc=0.886, est=0.859, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "  >acc=0.844, est=0.870, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "  >acc=0.865, est=0.864, cfg={'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "  >acc=0.853, est=0.865, cfg={'max_depth': 10, 'max_features': 36, 'n_estimators': 100}\n",
      "  >acc=0.870, est=0.857, cfg={'max_depth': 10, 'max_features': 16, 'n_estimators': 100}\n",
      "  Accuracy: 0.864 (0.014)\n",
      "  Training Master: {'max_depth': 10, 'max_features': 20, 'n_estimators': 100}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.856 \n",
      "\n",
      "  Starting data set 2...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "  >acc=0.896, est=0.883, cfg={'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "  >acc=0.892, est=0.882, cfg={'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "  >acc=0.894, est=0.888, cfg={'max_depth': 100, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "  >acc=0.904, est=0.884, cfg={'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "  >acc=0.864, est=0.886, cfg={'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "  Accuracy: 0.890 (0.014)\n",
      "  Training Master: {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.891 \n",
      "\n",
      "  Trial 2...\n",
      "  >acc=0.869, est=0.878, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "  >acc=0.882, est=0.884, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "  >acc=0.881, est=0.877, cfg={'max_depth': 100, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "  >acc=0.901, est=0.875, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "  >acc=0.891, est=0.882, cfg={'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "  Accuracy: 0.885 (0.011)\n",
      "  Training Master: {'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.901 \n",
      "\n",
      "  Trial 3...\n",
      "  >acc=0.878, est=0.887, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "  >acc=0.901, est=0.880, cfg={'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "  >acc=0.916, est=0.878, cfg={'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "  >acc=0.871, est=0.885, cfg={'max_depth': 100, 'max_features': 5, 'n_estimators': 100}\n",
      "  >acc=0.883, est=0.886, cfg={'max_depth': None, 'max_features': 8, 'n_estimators': 100}\n",
      "  Accuracy: 0.890 (0.016)\n",
      "  Training Master: {'max_depth': None, 'max_features': 5, 'n_estimators': 100}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.902 \n",
      "\n",
      "  Starting data set 3...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "  >acc=0.801, est=0.791, cfg={'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "  >acc=0.818, est=0.795, cfg={'max_depth': None, 'max_features': 18, 'n_estimators': 100}\n",
      "  >acc=0.797, est=0.804, cfg={'max_depth': None, 'max_features': 20, 'n_estimators': 100}\n",
      "  >acc=0.794, est=0.806, cfg={'max_depth': None, 'max_features': 36, 'n_estimators': 100}\n",
      "  >acc=0.805, est=0.801, cfg={'max_depth': 100, 'max_features': 18, 'n_estimators': 100}\n",
      "  Accuracy: 0.803 (0.008)\n",
      "  Training Master: {'max_depth': None, 'max_features': 18, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.822 \n",
      "\n",
      "  Trial 2...\n",
      "  >acc=0.810, est=0.816, cfg={'max_depth': None, 'max_features': 18, 'n_estimators': 100}\n",
      "  >acc=0.830, est=0.803, cfg={'max_depth': None, 'max_features': 2, 'n_estimators': 100}\n",
      "  >acc=0.808, est=0.812, cfg={'max_depth': None, 'max_features': 16, 'n_estimators': 100}\n",
      "  >acc=0.815, est=0.812, cfg={'max_depth': 100, 'max_features': 36, 'n_estimators': 100}\n",
      "  >acc=0.813, est=0.812, cfg={'max_depth': 100, 'max_features': 8, 'n_estimators': 100}\n",
      "  Accuracy: 0.815 (0.008)\n",
      "  Training Master: {'max_depth': None, 'max_features': 2, 'n_estimators': 100}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.813 \n",
      "\n",
      "  Trial 3...\n",
      "  >acc=0.832, est=0.810, cfg={'max_depth': None, 'max_features': 16, 'n_estimators': 100}\n",
      "  >acc=0.797, est=0.821, cfg={'max_depth': 100, 'max_features': 20, 'n_estimators': 100}\n",
      "  >acc=0.819, est=0.814, cfg={'max_depth': 100, 'max_features': 16, 'n_estimators': 100}\n",
      "  >acc=0.827, est=0.816, cfg={'max_depth': None, 'max_features': 8, 'n_estimators': 100}\n",
      "  >acc=0.806, est=0.813, cfg={'max_depth': 100, 'max_features': 16, 'n_estimators': 100}\n",
      "  Accuracy: 0.816 (0.013)\n",
      "  Training Master: {'max_depth': None, 'max_features': 16, 'n_estimators': 100}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.821 \n",
      "\n",
      "========================\n",
      "Starting: GaussianNB\n",
      "========================\n",
      "\n",
      "  Starting data set 1...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "  >acc=0.590, est=0.576, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.590, est=0.593, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.587, est=0.572, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.557, est=0.593, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.631, est=0.577, cfg={'var_smoothing': 1e-05}\n",
      "  Accuracy: 0.591 (0.024)\n",
      "  Training Master: {'var_smoothing': 1e-05}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.607 \n",
      "\n",
      "  Trial 2...\n",
      "  >acc=0.549, est=0.543, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.555, est=0.549, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.555, est=0.546, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.539, est=0.537, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.562, est=0.539, cfg={'var_smoothing': 1e-05}\n",
      "  Accuracy: 0.552 (0.008)\n",
      "  Training Master: {'var_smoothing': 1e-05}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.564 \n",
      "\n",
      "  Trial 3...\n",
      "  >acc=0.567, est=0.550, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.575, est=0.564, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.589, est=0.574, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.564, est=0.591, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.567, est=0.546, cfg={'var_smoothing': 1e-05}\n",
      "  Accuracy: 0.572 (0.009)\n",
      "  Training Master: {'var_smoothing': 1e-05}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.593 \n",
      "\n",
      "  Starting data set 2...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "  >acc=0.486, est=0.504, cfg={'var_smoothing': 1e-07}\n",
      "  >acc=0.606, est=0.625, cfg={'var_smoothing': 1e-07}\n",
      "  >acc=0.481, est=0.501, cfg={'var_smoothing': 1e-06}\n",
      "  >acc=0.470, est=0.510, cfg={'var_smoothing': 1e-07}\n",
      "  >acc=0.469, est=0.518, cfg={'var_smoothing': 1e-07}\n",
      "  Accuracy: 0.502 (0.052)\n",
      "  Training Master: {'var_smoothing': 1e-07}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.473 \n",
      "\n",
      "  Trial 2...\n",
      "  >acc=0.441, est=0.444, cfg={'var_smoothing': 1e-06}\n",
      "  >acc=0.551, est=0.546, cfg={'var_smoothing': 1e-07}\n",
      "  >acc=0.436, est=0.464, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.445, est=0.464, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.427, est=0.468, cfg={'var_smoothing': 1e-06}\n",
      "  Accuracy: 0.460 (0.046)\n",
      "  Training Master: {'var_smoothing': 1e-07}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.453 \n",
      "\n",
      "  Trial 3...\n",
      "  >acc=0.489, est=0.499, cfg={'var_smoothing': 1e-06}\n",
      "  >acc=0.478, est=0.496, cfg={'var_smoothing': 1e-07}\n",
      "  >acc=0.575, est=0.593, cfg={'var_smoothing': 1e-07}\n",
      "  >acc=0.478, est=0.499, cfg={'var_smoothing': 1e-07}\n",
      "  >acc=0.452, est=0.493, cfg={'var_smoothing': 1e-06}\n",
      "  Accuracy: 0.494 (0.042)\n",
      "  Training Master: {'var_smoothing': 1e-07}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.473 \n",
      "\n",
      "  Starting data set 3...\n",
      "  ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Trial 1...\n",
      "  >acc=0.607, est=0.617, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.643, est=0.617, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.620, est=0.624, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.626, est=0.625, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.617, est=0.620, cfg={'var_smoothing': 1e-05}\n",
      "  Accuracy: 0.623 (0.012)\n",
      "  Training Master: {'var_smoothing': 1e-05}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.623 \n",
      "\n",
      "  Trial 2...\n",
      "  >acc=0.620, est=0.623, cfg={'var_smoothing': 1e-06}\n",
      "  >acc=0.662, est=0.638, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.606, est=0.629, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.609, est=0.627, cfg={'var_smoothing': 1e-06}\n",
      "  >acc=0.634, est=0.620, cfg={'var_smoothing': 1e-05}\n",
      "  Accuracy: 0.626 (0.020)\n",
      "  Training Master: {'var_smoothing': 1e-05}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.628 \n",
      "\n",
      "  Trial 3...\n",
      "  >acc=0.665, est=0.645, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.664, est=0.651, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.604, est=0.652, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.626, est=0.633, cfg={'var_smoothing': 1e-05}\n",
      "  >acc=0.677, est=0.666, cfg={'var_smoothing': 1e-05}\n",
      "  Accuracy: 0.647 (0.028)\n",
      "  Training Master: {'var_smoothing': 1e-05}\n",
      "  Check Acc on Entire set\n",
      "  Final Acc = 0.649 \n",
      "\n",
      "CPU times: user 8min 17s, sys: 9.5 s, total: 8min 27s\n",
      "Wall time: 38min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifiers = [\n",
    "    {\n",
    "        'name': 'SVC',\n",
    "        'method': SVC,\n",
    "        'p_grid': {'C': [1,10,100,1000], 'gamma': [0.001,0.01,0.1,1.0]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'RandomForestClassifier',\n",
    "        'method': RandomForestClassifier,\n",
    "        'p_grid': {'n_estimators': [100], 'max_features': ['sqrt',2,5,8,16,18,20,36], 'max_depth': [10,100,None]}\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'name': 'GaussianNB',\n",
    "        'method': GaussianNB,\n",
    "        'p_grid': {'var_smoothing': [1.0e-5, 1.0e-6, 1.0e-7, 1.0e-8, 1.0e-9]}\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print('========================')\n",
    "    print('Starting: {}'.format(classifier['name']))\n",
    "    print('========================\\n')\n",
    "    \n",
    "    clf = classifier['method']()\n",
    "    p_grid = classifier['p_grid']\n",
    "    \n",
    "    for d_i in range(len(all_data)):\n",
    "        data_set = all_data[d_i]\n",
    "        \n",
    "        print('  Starting data set {}...'.format(d_i+1))\n",
    "        print('  ~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "        for i in range(NUM_TRIALS):\n",
    "\n",
    "            training_set, testing_set = get_training_test_sets(data_set, DATA_SIZE, pred_col='y')\n",
    "\n",
    "            print('  Trial {}...'.format(i+1))\n",
    "            trial_results = []\n",
    "            outer_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "            best_p = []\n",
    "            best_score = []\n",
    "\n",
    "            for tr_i, tst_i in outer_cv.split(training_set.X):\n",
    "                X_train, X_test = training_set.X[tr_i, :], training_set.X[tst_i, :]\n",
    "                y_train, y_test = training_set.y[tr_i], training_set.y[tst_i]\n",
    "\n",
    "                inner_cv = KFold(n_splits=FOLDS, shuffle=True, random_state=i)\n",
    "\n",
    "\n",
    "\n",
    "                search = GridSearchCV(\n",
    "                    estimator=clf,\n",
    "                    param_grid=p_grid,\n",
    "                    cv=inner_cv,\n",
    "                    verbose=0,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    refit=True\n",
    "                )\n",
    "\n",
    "                result = search.fit(X_train, y_train)\n",
    "\n",
    "                model = result.best_estimator_\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                best_p.append(result.best_params_)\n",
    "                best_score.append(acc)\n",
    "                trial_results.append(acc)\n",
    "\n",
    "                print('  >acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "\n",
    "\n",
    "            print('  Accuracy: %.3f (%.3f)' % (np.mean(trial_results), np.std(trial_results)))\n",
    "\n",
    "            run = best_score.index(max(best_score))\n",
    "            best_best_p = best_p[run]\n",
    "\n",
    "            oo_clf = classifier['method']()\n",
    "            print('  Training Master: %s' % best_best_p)\n",
    "            oo_clf.set_params(**best_best_p)\n",
    "            oo_clf.fit(training_set.X,training_set.y)\n",
    "\n",
    "            print('  Check Acc on Entire set')\n",
    "            y_pred = oo_clf.predict(testing_set.X)\n",
    "            acc = accuracy_score(testing_set.y, y_pred)\n",
    "            print('  Final Acc = %.3f \\n' % acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
